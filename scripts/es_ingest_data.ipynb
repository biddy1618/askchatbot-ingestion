{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL and ingestion of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "COLOR = 'white'\n",
    "plt.rcParams['text.color'       ] = COLOR\n",
    "plt.rcParams['text.color'       ] = COLOR\n",
    "plt.rcParams['axes.labelcolor'  ] = COLOR\n",
    "plt.rcParams['xtick.color'      ] = COLOR\n",
    "plt.rcParams['ytick.color'      ] = COLOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPM data - December 2021 Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PATH = Path('../data/uc-ipm/scrape_cleaned_Dec2021')\n",
    "DATA_FILE_NAMES = sorted(_PATH.iterdir())\n",
    "[data_file.name for data_file in DATA_FILE_NAMES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of files should be as following:\n",
    "```python\n",
    "['exoticPests.json',\n",
    " 'fruitItems_new.json',\n",
    " 'fruitVeggieEnvironItems_new.json',\n",
    " 'pestDiseaseItems_new.json',\n",
    " 'plantFlowerItems.json',\n",
    " 'turfPests.json',\n",
    " 'veggieItems_new.json',\n",
    " 'weedItems.json']\n",
    "```\n",
    "\n",
    "The corresponding ETL for these sources (links):\n",
    "* [`exoticPests.json`](#exoticpestsjson)\n",
    "* [`fruitItems_new.json`](#fruititems_newjson)\n",
    "* [`fruitVeggieEnvironItems_new.json`](#fruitveggieenvironitems_newjson)\n",
    "* [`pestDiseaseItems_new.json`](#pestdiseaseitems_newjson)\n",
    "* [`plantFlowerItems.json`](#plantfloweritemsjson)\n",
    "* [`turfPests.json`](#turfpestsjson)\n",
    "* [`veggieItems_new.json`](#veggieitems_newjson)\n",
    "* [`weedItems.json`](#weeditemsjson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Fix encodings and remove escape and redundant whitespace characters from text.\n",
    "    '''\n",
    "    text = text.encode('ascii', 'ignore').decode()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = html.unescape(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_text_from_fields(row, fields):\n",
    "    row_items = []\n",
    "    for field in fields:\n",
    "        text = row[field]\n",
    "\n",
    "        if len(text) == 0:\n",
    "            continue\n",
    "        \n",
    "        row_items.append({\n",
    "            'text': clean(text),\n",
    "            'field': field,\n",
    "            'name': field.replace('_', ' ').capitalize(),\n",
    "            'im_src': ''\n",
    "        })\n",
    "\n",
    "    return row_items\n",
    "\n",
    "def get_text_from_list_field(row, field, subfield, title = False, im_src = None):\n",
    "    row_items = []\n",
    "    for item in row[field]:\n",
    "        text = item[subfield]\n",
    "\n",
    "        if len(text) == 0:\n",
    "            continue\n",
    "\n",
    "        if title:\n",
    "            text = row['title'] + ' - ' + text\n",
    "        else: \n",
    "            text = text\n",
    "        \n",
    "        if im_src and len(item[im_src]) > 0:\n",
    "            src = item[im_src]\n",
    "        else: \n",
    "            src = ''\n",
    "        \n",
    "        row_items.append({\n",
    "            'text': clean(text),\n",
    "            'field': field,\n",
    "            'name': field.replace('_', ' ').capitalize(),\n",
    "            'im_src': src\n",
    "        })\n",
    "    \n",
    "    return row_items\n",
    "\n",
    "def get_images(row, field, im_src):\n",
    "    row_images = []\n",
    "    for item in row[field]:\n",
    "        if len(item[im_src]) > 0:\n",
    "            row_images.append(item[im_src])\n",
    "    \n",
    "    return row_images\n",
    "\n",
    "def transform_data(df, list_fields, list_text_fields, image_fields, limit = None):\n",
    "\n",
    "    if limit:\n",
    "        df = df.sample(limit).copy(deep=True)\n",
    "    \n",
    "    df['source'] = 'ucipm'\n",
    "    \n",
    "    for field in list_fields:\n",
    "        df[field] = df[field].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    \n",
    "    cols = [col for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)] if col not in ['url', 'source']]\n",
    "\n",
    "    texts = []\n",
    "    images = []\n",
    "    for _, row in df.iterrows():\n",
    "        row_texts = []\n",
    "        row_images = []\n",
    "        \n",
    "        row_texts.extend(get_text_from_fields(row, cols))\n",
    "\n",
    "        for field, subfield, concat_title, im_src in list_text_fields:\n",
    "            row_texts.extend(get_text_from_list_field(row, field, subfield, title=concat_title, im_src=im_src))\n",
    "        \n",
    "        for field, subfield in image_fields:\n",
    "            row_images.extend(get_images(row, field, subfield))\n",
    "\n",
    "        texts.append(row_texts)\n",
    "        images.append(row_images)\n",
    "    \n",
    "    df['texts'] = texts\n",
    "    df['images'] = images\n",
    "\n",
    "    df = df.loc[:, ['source', 'url', 'title', 'texts', 'images']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "\n",
    "FILE_NAME = 'exoticPests.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title'}, inplace = True)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images', 'related_links'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "        ('related_links', 'text', True, None),        \n",
    "    ],\n",
    "    image_fields=[('images', 'src')],\n",
    "    limit=30\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'fruitItems_new.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title'}, inplace = True)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['cultural_tips', 'pests_and_disorders'],\n",
    "    list_text_fields=[\n",
    "        ('cultural_tips', 'tip', True, None),\n",
    "        ('pests_and_disorders', 'problem', True, None),        \n",
    "    ],\n",
    "    image_fields=[],\n",
    "    limit=10\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'fruitVeggieEnvironItems_new.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title',}, inplace = True)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),        \n",
    "    ],\n",
    "    image_fields=[('images', 'src')],\n",
    "    limit=30\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'pestDiseaseItems_new.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title',}, inplace = True)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),        \n",
    "    ],\n",
    "    image_fields=[('images', 'src')],\n",
    "    limit=30\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'plantFlowerItems.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title',}, inplace = True)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images', 'pests_and_disorders'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "        ('pests_and_disorders', 'problem', True, None),        \n",
    "    ],\n",
    "    image_fields=[('images', 'src')],\n",
    "    limit=30\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'turfPests.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title',}, inplace = True)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),        \n",
    "    ],\n",
    "    image_fields=[('images', 'src')],\n",
    "    limit=30\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'veggieItems_new.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name'  : 'title'}, inplace = True)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images', 'pests_and_disorders'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "        ('pests_and_disorders', 'problem', True, None),        \n",
    "    ],\n",
    "    image_fields=[('images', 'src')],\n",
    "    limit=30\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'weedItems.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title',}, inplace = True)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, None),        \n",
    "    ],\n",
    "    image_fields=[],\n",
    "    limit=30\n",
    ")\n",
    "\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "print(f'Final shape is :{final_df.shape}')\n",
    "\n",
    "# pestsExotic().to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading embedding module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "os.environ['STAGE'          ] = 'dev'\n",
    "os.environ['ES_USERNAME'    ] = 'elastic'\n",
    "os.environ['ES_PASSWORD'    ] = 'changeme'\n",
    "os.environ['TF_CACHE_DIR'   ] = '/var/tmp/models'\n",
    "## select the environment for ingestion\n",
    "os.environ['ES_HOST'    ] = 'http://localhost:9200/'\n",
    "# os.environ['ES_HOST'    ] = 'https://dev.es.chat.ask.eduworks.com/'\n",
    "# os.environ['ES_HOST'    ] = 'https://qa.es.chat.ask.eduworks.com/'\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English \n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "def get_chunks(texts, max_seq_length):\n",
    "    texts_new = []\n",
    "    for item in texts:\n",
    "        text, field, name, im_src = item['text'], item['field'], item['name'], item['im_src']\n",
    "        \n",
    "        doc = nlp(text)\n",
    "        sents = [sent for sent in doc.sents]\n",
    "\n",
    "        start, end = 0, 1\n",
    "        while end != len(sents):\n",
    "            if start == end:\n",
    "                end +=1\n",
    "            elif len(' '.join([sent.text for sent in sents[start:end+1]])) > max_seq_length:\n",
    "                texts_new.append({\n",
    "                    'text': ' '.join([sent.text for sent in sents[start:end]]),\n",
    "                    'field': field,\n",
    "                    'name': name,\n",
    "                    'im_src': im_src\n",
    "                })\n",
    "                start += 1\n",
    "            else:\n",
    "                end += 1\n",
    "        texts_new.append({\n",
    "            'text': ' '.join([sent.text for sent in sents[start:end]]),\n",
    "            'field': field,\n",
    "            'name': name,\n",
    "            'im_src': im_src\n",
    "        })\n",
    "    return texts_new\n",
    "\n",
    "def embed_data(df, max_seq_size):\n",
    "    print(f'STARTING TRANSFORMING - MAX_SEQ_SIZE - {max_seq_size}')\n",
    "    df_texts = []\n",
    "    for i, row in df.iterrows():\n",
    "        row_texts = []\n",
    "        texts = row['texts']\n",
    "        df_texts.append(get_chunks(texts, max_seq_size))\n",
    "        if (i+1) % 500 == 0:\n",
    "            print(f'Finished transforming of {i+1} rows of dataframe')\n",
    "        \n",
    "        \n",
    "    print(f'Finished transforming of {i+1} rows of dataframe')\n",
    "    print(f'FINISHED TRANSFORMING')\n",
    "\n",
    "    texts = [item['text'] for row in df_texts for item in row]\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    print(f'STARTING EMBEDDING - BATCH_SIZE = {BATCH_SIZE}')\n",
    "    print(f'Number of texts to be embedded = {len(texts)}')\n",
    "\n",
    "    # TF HUB model\n",
    "    # vectors   = config.embed(texts_modified).numpy().tolist()\n",
    "        \n",
    "    # Sentence Encoder model        \n",
    "    vectors = config.embed.encode(\n",
    "        sentences           = texts     ,\n",
    "        batch_size          = BATCH_SIZE,\n",
    "        show_progress_bar   = True\n",
    "    ).tolist()\n",
    "\n",
    "    index = 0\n",
    "    for i, row in enumerate(df_texts):\n",
    "        for i1, item in enumerate(row):\n",
    "            item['vector'] = vectors[index]\n",
    "            item['field'] = item['field'] + str(i1)\n",
    "            assert texts[index] == item['text']\n",
    "            index += 1\n",
    "\n",
    "    print(f'FINISHED EMBEDDING')\n",
    "\n",
    "    df['texts'] = df_texts\n",
    "    print(f'The number of vectors to be ingested: {len([item[\"vector\"] for row in df[\"texts\"] for item in row])}', end='\\n\\n')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_SEQ_SIZE = config.embed.max_seq_length\n",
    "\n",
    "df = embed_data(final_df, MAX_SEQ_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting data into ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Final mapping__\n",
    "```json\n",
    "{\n",
    "    # mandatory fields\n",
    "    \"url\"       : \"url\",                            # Main URL\n",
    "    \"source\"    : \"ucipm|aekb|okstate|orstate\",     # Source Dataset\n",
    "    \"title\"     : \"title\",                          # Title of data item\n",
    "    \"images\"    : \"image_srcs\",                     # List of images sources if available\n",
    "    \"texts\"   : [\n",
    "        {\n",
    "        \"field\" : \"field_name_and_index\",           # Name of the field\n",
    "        \"name\"  : \"tab_name\",                       # Text for tab text\n",
    "        \"im_src\": \"image_source\",                   # If there is image to this text\n",
    "        \"text\"  : \"text_of_vector\",                 # Text of vector\n",
    "        \"vector\": \"dense_vector\",                   # Embedding vector\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different embedding sizes depending on the models\n",
    "# VECTOR_SIZE     = 384\n",
    "# VECTOR_SIZE     = 512\n",
    "VECTOR_SIZE     = 768\n",
    "MAX_STRING_SIZE = 32766\n",
    "\n",
    "mapping  = {\n",
    "    \"settings\": {\"number_of_shards\": 2, \"number_of_replicas\": 1},\n",
    "    \"mappings\": {\n",
    "        \"dynamic\"       : \"false\"   ,\n",
    "        \"date_detection\": \"false\"   ,\n",
    "        \"_source\"   : {\"enabled\": \"true\"},\n",
    "        \"properties\": {\n",
    "            \"source\"        : {\"type\": \"keyword\", \"index\": \"true\" , \"ignore_above\": MAX_STRING_SIZE},\n",
    "            \"url\"           : {\"type\": \"keyword\", \"index\": \"true\" , \"ignore_above\": MAX_STRING_SIZE},\n",
    "\n",
    "            \"title\"         : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": MAX_STRING_SIZE},\n",
    "            \"images\"        : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": MAX_STRING_SIZE},\n",
    "            \"vectors\"       : {\n",
    "                \"type\"      : \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"vector\": {\n",
    "                        \"type\": \"dense_vector\", \n",
    "                        \"dims\": VECTOR_SIZE\n",
    "                    },\n",
    "                    \"field\" : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": MAX_STRING_SIZE},\n",
    "                    \"name\"  : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": MAX_STRING_SIZE},\n",
    "                    \"im_src\": {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": MAX_STRING_SIZE},\n",
    "                    \"text\"  : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": MAX_STRING_SIZE},\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# increase the timeout if necessary\n",
    "es_client = Elasticsearch([config.es_host], http_auth=(config.es_username, config.es_password), timeout = 20)\n",
    "\n",
    "es_client.indices.delete(\n",
    "    index   = config.es_combined_index, \n",
    "    ignore  = 404)\n",
    "es_client.indices.create(\n",
    "    index       = config.es_combined_index  , \n",
    "    settings    = mapping['settings']       , \n",
    "    mappings    = mapping['mappings']       )\n",
    "# play with chunk size parameter for timed out problem\n",
    "final_json = df.to_dict(orient = 'records')\n",
    "deque(parallel_bulk(es_client, actions = final_json, index = config.es_combined_index, max_chunk_bytes = 5 * 1024 * 1024), maxlen = 0)\n",
    "\n",
    "es_client.indices.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPM data - April 2022 Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PATH = Path('../data/uc-ipm/scrape_cleaned_Apr2022/')\n",
    "DATA_FILE_NAMES = sorted(_PATH.iterdir())\n",
    "[data_file.name for data_file in DATA_FILE_NAMES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of files should be as following:\n",
    "```python\n",
    "['FruitVegCulturalItems.json',\n",
    " 'GardenControlsPestItems.json',\n",
    " 'GardenControlsPesticideItems.json',\n",
    " 'PestNotes.json',\n",
    " 'QuickTips.json',\n",
    " 'Videos.json',\n",
    " 'WeedIdItems.json']\n",
    "```\n",
    "\n",
    "The corresponding ETL for these sources (links):\n",
    "* [`FruitVegCulturalItems.json`](#fruitvegculturalitemsjson)\n",
    "* [`GardenControlsPestItems.json`](#gardercontolspestitemsjson)\n",
    "* [`GardenControlsPesticideItems.json`](#gardencontrolspesticideitemsjson)\n",
    "* [`PestNotes.json`](#pestnotesjson)\n",
    "* [`QuickTips.json`](#quicktipsjson)\n",
    "* [`Videos.json`](#videosjson)\n",
    "* [`WeedIdItems.json`](#weediditemsjson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_table(row):\n",
    "    '''\n",
    "    Rename the 'tips_table' key values to title with title and header concatenation.\n",
    "    '''\n",
    "    if len(row['tips_table']) > 0:\n",
    "        items = row['tips_table']\n",
    "        assert 'header' in items[0] \n",
    "        header_title = row['title'] + ' - ' + items[0]['header']\n",
    "        row['tips_table'] = header_title\n",
    "    else:\n",
    "        row['tips_table'] = ''\n",
    "\n",
    "def transform_pesticide(row):\n",
    "    '''\n",
    "    Merge pesticide subfield into main field - information.\n",
    "    '''\n",
    "    information = row['information'][0]\n",
    "    texts = []\n",
    "    for k, v in information.items():\n",
    "        texts.append(k.replace('_', ' ').capitalize() + ': ' + v + '. ')\n",
    "    row['information'] = '. '.join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "\n",
    "FILE_NAME = 'FruitVegCulturalItems.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title'}, inplace = True)\n",
    "df['tips_table'] = df['tips_table'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "df.apply(lambda r: transform_table(r), axis = 1)\n",
    "\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "    ],\n",
    "    image_fields=[('images', 'src')],\n",
    "    limit=30\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'GardenControlsPestItems.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title'}, inplace = True)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "    ],\n",
    "    image_fields=[('images', 'src')],\n",
    "    limit=10\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "FILE_NAME = 'GardenControlsPesticideItems.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "df['title'] = df[['active_ingredient', 'pesticide_type']].agg(' - '.join, axis=1)\n",
    "df.drop(['active_ingredient', 'pesticide_type'], axis=1, inplace=True)\n",
    "df.apply(lambda r: transform_pesticide(r), axis = 1)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=[],\n",
    "    list_text_fields=[],\n",
    "    image_fields=[],\n",
    "    limit=10\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'PestNotes.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "df.rename(columns = {\n",
    "    'urlPestNote'           : 'url'         ,\n",
    "    'name'                  : 'title'       ,\n",
    "    'descriptionPestNote'   : 'description' ,\n",
    "    'lifecyclePestNote'     : 'lifecycle'   ,\n",
    "    'damagePestNote'        : 'damage'      ,\n",
    "    'managementPestNote'    : 'management'  ,\n",
    "    'imagePestNote'         : 'images'      ,\n",
    "}, inplace = True)\n",
    "df.drop('tablePestNote', axis=1, inplace=True)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "    ],\n",
    "    image_fields=[('images', 'src')],\n",
    "    limit=30\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'QuickTips.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "df.rename(columns = {\n",
    "    'urlQuickTip'           : 'url'     ,\n",
    "    'name'                  : 'title'   ,\n",
    "    'contentQuickTips'      : 'content' ,\n",
    "    'imageQuickTips'        : 'images'  ,\n",
    "}, inplace = True)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "    ],\n",
    "    image_fields=[('images', 'src')],\n",
    "    limit=30\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'Videos.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=[],\n",
    "    list_text_fields=[],\n",
    "    image_fields=[],\n",
    "    limit=30\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'WeedIdItems.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name'  : 'title',}, inplace = True)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "    ],\n",
    "    image_fields=[('images', 'src')],\n",
    "    limit=10\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "print(f'Final shape is :{final_df.shape}')\n",
    "# df.to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_SIZE = config.embed.max_seq_length\n",
    "\n",
    "df = embed_data(final_df, MAX_SEQ_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting data into ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# increase the timeout if necessary\n",
    "es_client = Elasticsearch([config.es_host], http_auth=(config.es_username, config.es_password), timeout = 20)\n",
    "\n",
    "# play with chunk size parameter for timed out problem\n",
    "final_json = df.to_dict(orient = 'records')\n",
    "deque(parallel_bulk(es_client, actions = final_json, index = config.es_combined_index, max_chunk_bytes = 5 * 1024 * 1024), maxlen = 0)\n",
    "\n",
    "es_client.indices.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AskExtension Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PATH = Path('../data/askextension_kb/')\n",
    "DATA_FILE_NAMES = sorted(_PATH.iterdir())\n",
    "\n",
    "print(f'List of files:\\n{[data_file.name for data_file in DATA_FILE_NAMES]}')\n",
    "\n",
    "with open(DATA_FILE_NAMES[0]) as f:\n",
    "    f = json.load(f)\n",
    "    print(json.dumps(f[0], indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of files should be as following:\n",
    "```python\n",
    "['2012-2013.json', '2014-2015.json', '2016-2017.json', '2018-2019.json', '2020-1.json', '2020-2.json', '2021-1.json', '2021-2.json']\n",
    "```\n",
    "\n",
    "__NB__: We will only using tickets from California state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines the data files into one and returns it.\n",
    "df = pd.DataFrame()\n",
    "for f in DATA_FILE_NAMES:\n",
    "    df = pd.concat([df, pd.read_json(f)], ignore_index = True, axis = 0)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation as pn\n",
    "\n",
    "def transform_answer(answer_dict):\n",
    "    '''\n",
    "    Convert answer field from a dictionary to a list.\n",
    "    '''\n",
    "    answers = [{}] * len(answer_dict)\n",
    "    \n",
    "    for k, v in answer_dict.items():\n",
    "        # clean the response up\n",
    "        v = {\n",
    "            'response' : clean(v['response']),\n",
    "        }\n",
    "        answers[int(k) - 1] = v\n",
    "    \n",
    "    return answers\n",
    "\n",
    "def transform_title(title):\n",
    "    '''\n",
    "    Remove question ID from title, and append '.' in the end\n",
    "    if no punctuation was detected.\n",
    "\n",
    "    Example with '#' - 437259\n",
    "    Example with '...' - 437264\n",
    "    '''\n",
    "    title = ''.join(title.split('#')[:-1]).strip().strip('...')\n",
    "    \n",
    "    # add a '.' if it does not yet end with a punctuation\n",
    "    title = title if (title and title[-1] in pn) else title + '.'\n",
    "    \n",
    "    return title\n",
    "\n",
    "def merge_title_question(df):\n",
    "    '''\n",
    "    Create new column from questions and title,\n",
    "    but only if it is not already exactly in the question.\n",
    "    '''\n",
    "    titles      = df['title'    ].tolist()\n",
    "    questions   = df['question' ].tolist()\n",
    "    \n",
    "    tqs = [\n",
    "        question\n",
    "        if (title and question.startswith(title[:-1]))\n",
    "        else title + \" \" + question\n",
    "        for (title, question) in zip(titles, questions)\n",
    "    ]\n",
    "\n",
    "    return tqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify STATE_FILTER and MIN_WORD_COUNT variables accordingly\n",
    "# STATE_FILTER    = ['California', 'Oklahoma', 'Oregon']\n",
    "STATE_FILTER    = ['California']\n",
    "MIN_WORD_COUNT  = 3\n",
    "\n",
    "ASKEXTENSION_QUESTION_URL = 'https://ask2.extension.org/kb/faq.php?id='\n",
    "\n",
    "df['source'] = 'ae-kb'\n",
    "df['faq-id'] = df['faq-id'].astype(str)\n",
    "df = df[df['state'].isin(STATE_FILTER)]\n",
    "df['url'] = [\n",
    "    f\"{ASKEXTENSION_QUESTION_URL}{ticket_no}\" if len(ticket_no) == 6 else \"\"\n",
    "    for ticket_no in df['title'].str.split('#').str[-1]\n",
    "]\n",
    "df['ticket-no'] = [\n",
    "    ticket_no if len(ticket_no) == 6 else \"\"\n",
    "    for ticket_no in df['title'].str.split('#').str[-1]\n",
    "]\n",
    "df['attachments'] = df['attachments'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "df['attachments'] = df['attachments'].apply(lambda d: [{'src': link} for link in d])\n",
    "df.rename(columns = {'faq-id': 'faq_id', 'ticket-no': 'ticket_no'}, inplace = True)\n",
    "\n",
    "df['answers'] = df['answer'].apply(transform_answer)\n",
    "df['title'] = df['title'].apply(transform_title)\n",
    "df['question'] = merge_title_question(df)\n",
    "\n",
    "if MIN_WORD_COUNT:\n",
    "    df = df[df['question'].str.split().str.len() > MIN_WORD_COUNT]\n",
    "\n",
    "df = df.loc[:, ['source', 'url', 'title', 'question', 'answers', 'attachments']]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['answers',],\n",
    "    list_text_fields=[\n",
    "        ('answers', 'response', False, None),\n",
    "    ],\n",
    "    image_fields=[('attachments', 'src')],\n",
    "    limit=30\n",
    ")\n",
    "\n",
    "print(f'Shape of data: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_SIZE = config.embed.max_seq_length\n",
    "\n",
    "df = embed_data(df, MAX_SEQ_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting data into ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# increase the timeout if necessary\n",
    "es_client = Elasticsearch([config.es_host], http_auth=(config.es_username, config.es_password), timeout = 20)\n",
    "\n",
    "# play with chunk size parameter for timed out problem\n",
    "final_json = df.to_dict(orient = 'records')\n",
    "deque(parallel_bulk(es_client, actions = final_json, index = config.es_combined_index, max_chunk_bytes = 5 * 1024 * 1024), maxlen = 0)\n",
    "\n",
    "es_client.indices.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oklahoma State University data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PATH = Path('../data/okstate/fact-sheets-out-cleaner.json')\n",
    "df = pd.read_json(_PATH)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def get_title_and_description(row, thumbnail = False):\n",
    "    title = clean(row['title'])\n",
    "    description = clean(row['description'])\n",
    "    if thumbnail:\n",
    "        im_src = row['thumbnail']\n",
    "    else:\n",
    "        im_src = ''\n",
    "\n",
    "    texts = []\n",
    "    texts.append({\n",
    "        'text': title,\n",
    "        'field': 'title',\n",
    "        'name': 'Title',\n",
    "        'im_src': im_src\n",
    "    })\n",
    "    if len(description) > 0:\n",
    "        texts.append({\n",
    "            'text': description,\n",
    "            'field': 'description',\n",
    "            'name': 'Description',\n",
    "            'im_src': im_src\n",
    "        })\n",
    "\n",
    "    return texts\n",
    "\n",
    "def get_contents_and_images(row, thumbnail=False):\n",
    "    '''\n",
    "    Transform the content field by concatenating title with header, and perform cleaning. Drop the unncessary columns.\n",
    "    '''\n",
    "    title = clean(row['title'])\n",
    "    if thumbnail:\n",
    "        im_src = row['thumbnail']\n",
    "    else:\n",
    "        im_src = ''\n",
    "\n",
    "    texts = []\n",
    "    images = []\n",
    "\n",
    "    if thumbnail:\n",
    "        images.append(im_src)\n",
    "    for content in row['content']:\n",
    "        item = {}\n",
    "        header = clean(content['header'])\n",
    "        if len(header) == 0 or header == 'Introduction-w/o-header':\n",
    "            header = clean(title)\n",
    "        else:\n",
    "            header = clean(title + ' - ' + header)\n",
    "        item['text'] = header\n",
    "        item['field'] = 'content'\n",
    "        item['name'] = 'Paragraph'\n",
    "        if len(content['images']['image_urls']) > 0:\n",
    "            item['im_src'] = content['images']['image_urls'][0]\n",
    "        elif thumbnail:\n",
    "            item['im_src'] = im_src\n",
    "        else:\n",
    "            item['im_src'] = ''\n",
    "        texts.append(item)\n",
    "        \n",
    "        text = clean(content['text'])\n",
    "        if len(text) > 0:\n",
    "            item = deepcopy(item)\n",
    "            item['text'] = clean(content['text'])\n",
    "        \n",
    "            texts.append(item)\n",
    "        \n",
    "        for url, caption in zip(content['images']['image_urls'], content['images']['image_captions']):\n",
    "            item = {}\n",
    "            if len(caption) > 0:\n",
    "                item['text'] = clean(header + ' - ' + caption)\n",
    "                item['field'] = 'image'\n",
    "                item['name'] = 'Image'\n",
    "                item['im_src'] = url\n",
    "                texts.append(item)\n",
    "            if len(url) > 0:\n",
    "                images.append(url)\n",
    "    \n",
    "    return texts, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'] = 'okstate'\n",
    "\n",
    "df['title'] = df['title'].apply(clean).fillna('Auxilary')\n",
    "df.rename(columns={'link': 'url'}, inplace=True)\n",
    "df.drop(columns=['author', 'pubdate', 'category', 'displaydate'], inplace=True)\n",
    "\n",
    "texts = []\n",
    "images = []\n",
    "for _, row in df.iterrows():\n",
    "    title_description_texts = get_title_and_description(row, thumbnail=True)\n",
    "    row_texts, row_images = get_contents_and_images(row, thumbnail=True)\n",
    "    title_description_texts.extend(row_texts)\n",
    "    \n",
    "    texts.append(row_texts)\n",
    "    images.append(row_images)\n",
    "\n",
    "df['texts'] = texts\n",
    "df['images'] = images\n",
    "\n",
    "df = df.loc[:, ['source', 'url', 'title', 'texts', 'images']]\n",
    "df = df.sample(50).reset_index(drop=True)\n",
    "\n",
    "print(f'The data shape: {df.shape}')\n",
    "df.to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_SEQ_SIZE = config.embed.max_seq_length\n",
    "\n",
    "df = embed_data(df, MAX_SEQ_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting data into ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# increase the timeout if necessary\n",
    "es_client = Elasticsearch([config.es_host], http_auth=(config.es_username, config.es_password), timeout = 20)\n",
    "\n",
    "# play with chunk size parameter for timed out problem\n",
    "final_json = df.to_dict(orient = 'records')\n",
    "deque(parallel_bulk(es_client, actions = final_json, index = config.es_combined_index, max_chunk_bytes = 5 * 1024 * 1024), maxlen = 0)\n",
    "\n",
    "es_client.indices.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oregon State University data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PATH = Path('../data/orstate/OSU-Out-Cleaner.json')\n",
    "df = pd.read_json(_PATH)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'] = 'orstate'\n",
    "\n",
    "df['title'] = df['title'].apply(clean).fillna('Auxilary')\n",
    "df.rename(columns={'link': 'url'}, inplace=True)\n",
    "df.drop(columns=['author', 'pubdate', 'category', 'displaydate'], inplace=True)\n",
    "\n",
    "texts = []\n",
    "images = []\n",
    "for i, row in df.iterrows():\n",
    "    title_description_texts = get_title_and_description(row)\n",
    "    row_texts, row_images = get_contents_and_images(row)\n",
    "    title_description_texts.extend(row_texts)\n",
    "    \n",
    "    texts.append(row_texts)\n",
    "    images.append(row_images)\n",
    "\n",
    "df['texts'] = texts\n",
    "df['images'] = images\n",
    "\n",
    "df = df.loc[:, ['source', 'url', 'title', 'texts', 'images']]\n",
    "df = df.sample(50).reset_index(drop=True)\n",
    "\n",
    "print(f'The data shape: {df.shape}')\n",
    "df.to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_SIZE = config.embed.max_seq_length\n",
    "\n",
    "df = embed_data(df, MAX_SEQ_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting data into ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# increase the timeout if necessary\n",
    "es_client = Elasticsearch([config.es_host], http_auth=(config.es_username, config.es_password), timeout = 20)\n",
    "\n",
    "# play with chunk size parameter for timed out problem\n",
    "final_json = df.to_dict(orient = 'records')\n",
    "deque(parallel_bulk(es_client, actions = final_json, index = config.es_combined_index, max_chunk_bytes = 5 * 1024 * 1024), maxlen = 0)\n",
    "\n",
    "es_client.indices.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Jeff's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE      = 1\n",
    "ROLLING_SIZE    = 3\n",
    "MAX_SEQ_SIZE    = config.embed.max_seq_length\n",
    "FINAL_COLS      = ['source', 'url', 'title', \"vector\"]\n",
    "\n",
    "\n",
    "for file_name, df in data:\n",
    "\n",
    "    texts = [r['title'] for i, r in df.iterrows()]\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    print(f'STARTING EMBEDDING - BATCH_SIZE = {BATCH_SIZE}')\n",
    "    print(f'Number of texts to be embedded = {len(texts)}')\n",
    "    df['vectors'] = np.empty((len(df), 0)).tolist()\n",
    "\n",
    "    # TF HUB model\n",
    "    # vectors   = config.embed(texts_modified).numpy().tolist()\n",
    "        \n",
    "    # Sentence Encoder model        \n",
    "    \n",
    "    vectors = config.embed.encode(\n",
    "        sentences           = texts     ,\n",
    "        batch_size          = BATCH_SIZE,\n",
    "        show_progress_bar   = True\n",
    "    ).tolist()\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    print(f'FINISHED EMBEDDING')\n",
    "\n",
    "    df['vector'] = vectors\n",
    "    df = df.loc[:, FINAL_COLS]\n",
    "    print(f'The number of vectors to be ingested: {len(df[\"vector\"])}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Final mapping__\n",
    "```json\n",
    "{\n",
    "    # mandatory fields\n",
    "    \"url\"       : \"url\",                            # Main URL\n",
    "    \"source\"    : \"ucipm|aekb|okstate|orstate\",     # Source Dataset\n",
    "    \"title\"     : \"title\",                          # Title of data item\n",
    "    \"vectors\"   : \"dense_vector\",                   # Embedding vector\n",
    "},\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different embedding sizes depending on the models\n",
    "# VECTOR_SIZE     = 384\n",
    "# VECTOR_SIZE     = 512\n",
    "VECTOR_SIZE     = 768\n",
    "MAX_STRING_SIZE = 32766\n",
    "\n",
    "mapping  = {\n",
    "    \"settings\": {\"number_of_shards\": 2, \"number_of_replicas\": 1},\n",
    "    \"mappings\": {\n",
    "        \"dynamic\"       : \"false\"   ,\n",
    "        \"date_detection\": \"false\"   ,\n",
    "        \"_source\"   : {\"enabled\": \"true\"},\n",
    "        \"properties\": {\n",
    "            \"source\"        : {\"type\": \"keyword\", \"index\": \"true\" , \"ignore_above\": MAX_STRING_SIZE},\n",
    "            \"url\"           : {\"type\": \"keyword\", \"index\": \"true\" , \"ignore_above\": MAX_STRING_SIZE},\n",
    "\n",
    "            \"title\"         : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": MAX_STRING_SIZE},\n",
    "            \"vector\"        : {\n",
    "                        \"type\": \"dense_vector\", \n",
    "                        \"dims\": VECTOR_SIZE\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# increase the timeout if necessary\n",
    "es_client = Elasticsearch([config.es_host], http_auth=(config.es_username, config.es_password), timeout = 20)\n",
    "\n",
    "es_client.indices.delete(\n",
    "    index   = config.es_combined_index, \n",
    "    ignore  = 404)\n",
    "es_client.indices.create(\n",
    "    index       = config.es_combined_index  , \n",
    "    settings    = mapping['settings']       , \n",
    "    mappings    = mapping['mappings']       )\n",
    "# play with chunk size parameter for timed out problem\n",
    "for file_name, df in data:\n",
    "    print(f'Ingesting \"{file_name}\"...')\n",
    "    final_json = df.to_dict(orient = 'records')\n",
    "    deque(parallel_bulk(es_client, actions = final_json, index = config.es_combined_index, max_chunk_bytes = 5 * 1024 * 1024), maxlen = 0)\n",
    "\n",
    "es_client.indices.refresh()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('es-data-ingestion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "607e550c5f7577ecefb8f11c45030a36424a53b9c08a45019262888187990ca7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
