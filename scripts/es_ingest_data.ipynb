{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL and ingestion of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "COLOR = 'white'\n",
    "plt.rcParams['text.color'       ] = COLOR\n",
    "plt.rcParams['text.color'       ] = COLOR\n",
    "plt.rcParams['axes.labelcolor'  ] = COLOR\n",
    "plt.rcParams['xtick.color'      ] = COLOR\n",
    "plt.rcParams['ytick.color'      ] = COLOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPM data - December 2021 Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PATH = Path('../data/uc-ipm/scrape_cleaned_Dec2021')\n",
    "DATA_FILE_NAMES = sorted(_PATH.iterdir())\n",
    "[data_file.name for data_file in DATA_FILE_NAMES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of files should be as following:\n",
    "```python\n",
    "['exoticPests.json',\n",
    " 'fruitItems_new.json',\n",
    " 'fruitVeggieEnvironItems_new.json',\n",
    " 'pestDiseaseItems_new.json',\n",
    " 'plantFlowerItems.json',\n",
    " 'turfPests.json',\n",
    " 'veggieItems_new.json',\n",
    " 'weedItems.json']\n",
    "```\n",
    "\n",
    "The corresponding ETL for these sources (links):\n",
    "* [`exoticPests.json`](#exoticpestsjson)\n",
    "* [`fruitItems_new.json`](#fruititems_newjson)\n",
    "* [`fruitVeggieEnvironItems_new.json`](#fruitveggieenvironitems_newjson)\n",
    "* [`pestDiseaseItems_new.json`](#pestdiseaseitems_newjson)\n",
    "* [`plantFlowerItems.json`](#plantfloweritemsjson)\n",
    "* [`turfPests.json`](#turfpestsjson)\n",
    "* [`veggieItems_new.json`](#veggieitems_newjson)\n",
    "* [`weedItems.json`](#weeditemsjson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_key(row, field, subfield):\n",
    "    '''\n",
    "    Add the title to the subfield text.\n",
    "    '''\n",
    "    for link in row[field]:\n",
    "        link['title'] = link.pop(subfield)\n",
    "        if len(link['title']) > 0:\n",
    "            link['title'] = row['title'] + ' - ' + link['title']\n",
    "\n",
    "def clean(text):\n",
    "    '''\n",
    "    Fix encodings and remove escape and redundant whitespace characters from text.\n",
    "    '''\n",
    "    text = text.encode('ascii', 'ignore').decode()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `exoticPests.json`\n",
    "<a id='exoticpestsjson'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pestsExotic():\n",
    "    # -------------------------------------------- Exotic pests\n",
    "    FILE_NAME = 'exoticPests.json'\n",
    "    print(f'Transforming \"{FILE_NAME}\"...')\n",
    "    df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "    \n",
    "    df['source'] = 'ucipm'\n",
    "\n",
    "    df.rename(columns = {'name': 'title'}, inplace = True)\n",
    "\n",
    "    df['images'] = df['images'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'images', 'caption'), axis = 1)\n",
    "    \n",
    "    df['related_links'] = df['related_links'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'related_links', 'text'), axis = 1)\n",
    "    \n",
    "    for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)]:\n",
    "        df[col] = df[col].apply(clean)\n",
    "\n",
    "    return df\n",
    "\n",
    "pestsExotic().to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fruitItems_new.json`\n",
    "<a id='fruititems_newjson'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infoFruits():\n",
    "    # -------------------------------------------- Fruits information\n",
    "    FILE_NAME = 'fruitItems_new.json'\n",
    "    print(f'Transforming \"{FILE_NAME}\"...')\n",
    "    df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "    \n",
    "    df['source'] = 'ucipm'\n",
    "    \n",
    "    df.rename(columns = {'name': 'title'}, inplace = True)\n",
    "\n",
    "    df['cultural_tips'] = df['cultural_tips'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'cultural_tips', 'tip'), axis = 1)\n",
    "    \n",
    "    df['pests_and_disorders'] = df['pests_and_disorders'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'pests_and_disorders', 'problem'), axis = 1)\n",
    "\n",
    "    for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)]:\n",
    "        df[col] = df[col].apply(clean)\n",
    "\n",
    "    return df\n",
    "\n",
    "infoFruits().to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fruitVeggieEnvironItems_new.json`\n",
    "<a id='fruitveggieenvironitems_newjson'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def damagesEnvironment():\n",
    "    # -------------------------------------------- Fruit and veggie damages\n",
    "    FILE_NAME = 'fruitVeggieEnvironItems_new.json'\n",
    "    print(f'Transforming \"{FILE_NAME}\"...')\n",
    "    df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "    \n",
    "    df['source'] = 'ucipm'\n",
    "\n",
    "    df.rename(columns = {'name': 'title',}, inplace = True)\n",
    "\n",
    "    df['images'] = df['images'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'images', 'caption'), axis = 1)\n",
    "\n",
    "\n",
    "    for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)]:\n",
    "        df[col] = df[col].apply(clean)\n",
    "\n",
    "    return df\n",
    "\n",
    "damagesEnvironment().to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pestDiseaseItems_new.json`\n",
    "<a id='pestdiseaseitems_newjson'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pestsDiseases():\n",
    "    # -------------------------------------------- Pests diseases\n",
    "    FILE_NAME = 'pestDiseaseItems_new.json'\n",
    "    print(f'Transforming \"{FILE_NAME}\"...')\n",
    "    df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "    \n",
    "    df['source'] = 'ucipm'\n",
    "\n",
    "    df.rename(columns = {'name': 'title'}, inplace = True)\n",
    "\n",
    "    df['images'] = df['images'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'images', 'caption'), axis = 1)\n",
    "\n",
    "\n",
    "    for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)]:\n",
    "        df[col] = df[col].apply(clean)\n",
    "\n",
    "    return df\n",
    "\n",
    "pestsDiseases().to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `plantFlowerItems.json`\n",
    "<a id='plantfloweritemsjson'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infoFlowers():\n",
    "    # -------------------------------------------- Flowers information\n",
    "    FILE_NAME = 'plantFlowerItems.json'\n",
    "    print(f'Transforming \"{FILE_NAME}\"...')\n",
    "    df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "\n",
    "    df['source'] = 'ucipm'\n",
    "\n",
    "    df.rename(columns = {'name': 'title',}, inplace = True)\n",
    "\n",
    "    df['images'] = df['images'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'images', 'caption'), axis = 1)\n",
    "\n",
    "    df['pests_and_disorders'] = df['pests_and_disorders'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'pests_and_disorders', 'problem'), axis = 1)\n",
    "\n",
    "    for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)]:\n",
    "        df[col] = df[col].apply(clean)\n",
    "\n",
    "    return df\n",
    "\n",
    "infoFlowers().to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `turfPests.json`\n",
    "<a id='plantfloweritemsjson'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pestsTurf():\n",
    "    # -------------------------------------------- Turf pests\n",
    "    FILE_NAME = 'turfPests.json'\n",
    "    print(f'Transforming \"{FILE_NAME}\"...')\n",
    "    df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "\n",
    "    df['source'] = 'ucipm'\n",
    "\n",
    "    df.rename(columns = {'name': 'title'}, inplace = True)\n",
    "\n",
    "    df['images'] = df['images'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'images', 'caption'), axis = 1)\n",
    "\n",
    "    for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)]:\n",
    "        df[col] = df[col].apply(clean)\n",
    "\n",
    "    return df\n",
    "\n",
    "pestsTurf().to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `veggieItems_new.json`\n",
    "<a id='veggieitems_newjson'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infoVeggies():\n",
    "    # -------------------------------------------- Veggies information\n",
    "    FILE_NAME = 'veggieItems_new.json'\n",
    "    print(f'Transforming \"{FILE_NAME}\"...')\n",
    "    df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "\n",
    "    df['source'] = 'ucipm'\n",
    "\n",
    "    df.rename(columns = {'name'  : 'title'}, inplace = True)\n",
    "\n",
    "    df['images'] = df['images'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'images', 'caption'), axis = 1)\n",
    "\n",
    "    df['pests_and_disorders'] = df['pests_and_disorders'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'pests_and_disorders', 'problem'), axis = 1)\n",
    "\n",
    "    for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)]:\n",
    "        df[col] = df[col].apply(clean)\n",
    "\n",
    "    return df\n",
    "\n",
    "infoVeggies().to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `weedItems.json`\n",
    "<a id='weeditemsjson'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def damagesWeed():\n",
    "    # -------------------------------------------- Weed damages\n",
    "    FILE_NAME = 'weedItems.json'\n",
    "    print(f'Transforming \"{FILE_NAME}\"...')\n",
    "    df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "\n",
    "    df['source'] = 'ucipm'\n",
    "    \n",
    "    df.rename(columns = {'name': 'title'}, inplace = True)\n",
    "\n",
    "\n",
    "    df['images'] = df['images'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'images', 'caption'), axis = 1)\n",
    "\n",
    "    for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)]:\n",
    "        df[col] = df[col].apply(clean)\n",
    "\n",
    "    return df\n",
    "\n",
    "damagesWeed().to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging and transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "os.environ['STAGE'          ] = 'dev'\n",
    "os.environ['ES_USERNAME'    ] = 'elastic'\n",
    "os.environ['ES_PASSWORD'    ] = 'changeme'\n",
    "os.environ['TF_CACHE_DIR'   ] = '/var/tmp/models'\n",
    "## select the environment for ingestion\n",
    "os.environ['ES_HOST'    ] = 'http://localhost:9200/'\n",
    "# os.environ['ES_HOST'    ] = 'https://dev.es.chat.ask.eduworks.com/'\n",
    "# os.environ['ES_HOST'    ] = 'https://qa.es.chat.ask.eduworks.com/'\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('exoticPests.json', pestsExotic()),\n",
    "    ('fruitItems_new.json', infoFruits()),\n",
    "    ('fruitVeggieEnvironItems_new.json', damagesEnvironment()),\n",
    "    ('pestsDiseaseItems_new.json', pestsDiseases()),\n",
    "    ('plantFlowerItems.json', infoFlowers()),\n",
    "    ('turfPests.json', pestsTurf()),\n",
    "    ('veggieItems_new.json', infoVeggies()),\n",
    "    ('weedItems.json', damagesWeed())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE      = 1\n",
    "ROLLING_SIZE    = 3\n",
    "\n",
    "for file_name, df in data:\n",
    "\n",
    "    print(f'File name - \"{file_name}\"...')\n",
    "\n",
    "    cols = [col for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)] if col not in ['url', 'source']]\n",
    "    list_cols = df.columns[df.applymap(lambda x: isinstance(x, list)).all(0)].values\n",
    "\n",
    "    print(f'Transforming columns - text {cols} and links {list_cols}].')\n",
    "\n",
    "    from spacy.lang.en import English \n",
    "\n",
    "    nlp = English()\n",
    "    nlp.add_pipe('sentencizer')\n",
    "\n",
    "    print(f'STARTING TRANSFORMING - CHUNK_SIZE - {CHUNK_SIZE}, ROLLING_SIZE = {ROLLING_SIZE}')\n",
    "    c_items = []\n",
    "    for i, r in df.iterrows():\n",
    "        r_texts = []\n",
    "\n",
    "        n_sentences = 0\n",
    "        for c in cols:\n",
    "            t = r[c]\n",
    "            \n",
    "            doc = nlp(t)\n",
    "            \n",
    "            ts = [sent for sent in doc.sents]\n",
    "            if len(ts) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                chunks, chunk_size, roll_size = len(ts), CHUNK_SIZE, ROLLING_SIZE\n",
    "                ts = [ts[i1:i1+chunk_size+(roll_size - 1)] for i1 in range(0, chunks - (roll_size - 1), chunk_size)]\n",
    "                ts = [{'text': ' '.join([l2.text for l2 in l1]), 'name': c + '_' + str(i1), 'start': l1[0].start_char, 'end': l1[-1].end_char} for i1, l1 in enumerate(ts)]\n",
    "            \n",
    "            n_sentences += len(ts)\n",
    "            r_texts.extend(ts)\n",
    "        \n",
    "        links_exits = False\n",
    "        for lc in list_cols:\n",
    "            ts = [i1['title'] for i1 in r[lc]]\n",
    "            for i1, v in enumerate(ts):\n",
    "                r_texts.append({'text': v, 'name': lc + '.' + 'title' + '_' + str(i1), 'start': 0, 'end': -1})\n",
    "        \n",
    "        \n",
    "        c_items.append(r_texts)\n",
    "        # break\n",
    "        if (i+1) % 500 == 0:\n",
    "            print(f'Finished transforming of {i+1} rows of dataframe')\n",
    "\n",
    "    print(f'Finished transforming of {i+1} rows of dataframe')\n",
    "    print(f'FINISHED TRANSFORMING')\n",
    "\n",
    "    texts = [r1['text'] for r in c_items for r1 in r]\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    print(f'STARTING EMBEDDING - BATCH_SIZE = {BATCH_SIZE}')\n",
    "    df['vectors'] = np.empty((len(df), 0)).tolist()\n",
    "\n",
    "    # TF HUB model\n",
    "    # vectors   = config.embed(texts_modified).numpy().tolist()\n",
    "        \n",
    "    # Sentence Encoder model        \n",
    "    vectors = config.embed.encode(\n",
    "        sentences           = texts     ,\n",
    "        batch_size          = BATCH_SIZE,\n",
    "        show_progress_bar   = True\n",
    "    ).tolist()\n",
    "\n",
    "    index = 0\n",
    "    for i, r in enumerate(c_items):\n",
    "        for i1, r1 in enumerate(r):\n",
    "            r1['vector'] = vectors[index]\n",
    "            r1.pop('text')\n",
    "            index += 1\n",
    "\n",
    "    print(f'FINISHED EMBEDDING')\n",
    "\n",
    "    df['vectors'] = c_items\n",
    "    print(f'The number of vectors to be ingested: {len([r1[\"vector\"] for r in df[\"vectors\"] for r1 in r])}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting data into ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Final mapping__\n",
    "```json\n",
    "{\n",
    "    # mandatory fields\n",
    "    \"url\"       : \"url\",                            # Main URL\n",
    "    \"source\"    : \"ucipm|aekb|okstate|orstate\",     # Source Dataset\n",
    "    \"title\"     : \"title\",                          # Title of data item\n",
    "    ...\n",
    "    # other fields\n",
    "    ...\n",
    "    \"vectors\"   : {\n",
    "        \"name\"  : \"field_name_and_index\",           # Name of the field\n",
    "        \"start\" : \"number\",                         # Start index within text\n",
    "        \"end\"   : \"number\",                         # End index within text\n",
    "        \"vector\": \"dense_vector\",                   # Embedding vector\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different embedding sizes depending on the models\n",
    "# VECTOR_SIZE = 384\n",
    "# VECTOR_SIZE = 512\n",
    "VECTOR_SIZE = 768\n",
    "\n",
    "mapping  = {\n",
    "    \"settings\": {\"number_of_shards\": 2, \"number_of_replicas\": 1},\n",
    "    \"mappings\": {\n",
    "        \"dynamic\"   : \"true\",\n",
    "        \"_source\"   : {\"enabled\": \"true\"},\n",
    "        \"properties\": {\n",
    "            \"source\"        : {\"type\": \"keyword\", \"index\": \"true\" , \"ignore_above\": 32766},\n",
    "            \"url\"           : {\"type\": \"keyword\", \"index\": \"true\" , \"ignore_above\": 32766},\n",
    "\n",
    "            \"title\"         : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": 32766},\n",
    "            \"vectors\"       : {\n",
    "                \"type\"      : \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"vector\": {\n",
    "                        \"type\": \"dense_vector\", \n",
    "                        \"dims\": VECTOR_SIZE\n",
    "                    },\n",
    "                    \"name\"  : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": 32766},\n",
    "                    \"start\" : {\"type\": \"integer\"                                         },\n",
    "                    \"end\"   : {\"type\": \"integer\"                                         },\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# increase the timeout if necessary\n",
    "es_client = Elasticsearch([config.es_host], http_auth=(config.es_username, config.es_password), timeout = 20)\n",
    "\n",
    "es_client.indices.delete(\n",
    "    index   = config.es_combined_index, \n",
    "    ignore  = 404)\n",
    "es_client.indices.create(\n",
    "    index       = config.es_combined_index  , \n",
    "    settings    = mapping['settings']       , \n",
    "    mappings    = mapping['mappings']       )\n",
    "# play with chunk size parameter for timed out problem\n",
    "for file_name, df in data:\n",
    "    print(f'Ingesting \"{file_name}\"...')\n",
    "    final_json = df.to_dict(orient = 'records')\n",
    "    deque(parallel_bulk(es_client, actions = final_json, index = config.es_combined_index, max_chunk_bytes = 5 * 1024 * 1024), maxlen = 0)\n",
    "\n",
    "es_client.indices.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPM data - April 2022 Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PATH = Path('../data/uc-ipm/scrape_cleaned_Apr2022/')\n",
    "DATA_FILE_NAMES = sorted(_PATH.iterdir())\n",
    "[data_file.name for data_file in DATA_FILE_NAMES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of files should be as following:\n",
    "```python\n",
    "['FruitVegCulturalItems.json',\n",
    " 'GardenControlsPestItems.json',\n",
    " 'GardenControlsPesticideItems.json',\n",
    " 'PestNotes.json',\n",
    " 'QuickTips.json',\n",
    " 'Videos.json',\n",
    " 'WeedIdItems.json']\n",
    "```\n",
    "\n",
    "The corresponding ETL for these sources (links):\n",
    "* [`FruitVegCulturalItems.json`](#fruitvegculturalitemsjson)\n",
    "* [`GardenControlsPestItems.json`](#gardercontolspestitemsjson)\n",
    "* [`GardenControlsPesticideItems.json`](#gardencontrolspesticideitemsjson)\n",
    "* [`PestNotes.json`](#pestnotesjson)\n",
    "* [`QuickTips.json`](#quicktipsjson)\n",
    "* [`Videos.json`](#videosjson)\n",
    "* [`WeedIdItems.json`](#weediditemsjson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_table(row):\n",
    "    '''\n",
    "    Rename the 'tips_table' key values to title with title and header concatenation.\n",
    "    '''\n",
    "    if len(row['tips_table']) > 0:\n",
    "        items = row['tips_table']\n",
    "        assert 'header' in items[0]\n",
    "        header = items[0]['header']\n",
    "        header_title = row['title'] + ' - ' + items[0].pop('header')\n",
    "        items[0]['title'] = header_title\n",
    "        for item in items[1:]:\n",
    "            item['title'] = item.pop('row')\n",
    "            if len(item['title']) > 0:\n",
    "                item['title'] = header_title + ' - ' + item['title']\n",
    "\n",
    "def transform_pesticide(row):\n",
    "    '''\n",
    "    Merge pesticide subfield into main field - information.\n",
    "    '''\n",
    "    information = row['information'][0]\n",
    "    texts = []\n",
    "    for k, v in information.items():\n",
    "        texts.append(k.replace('_', ' ').capitalize() + ': ' + v)\n",
    "    row['information'] = '. '.join(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `FruitVegCulturalItems.json`\n",
    "<a id='fruitvegculturalitemsjson'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infoFruitVegCultural():\n",
    "    # -------------------------------------------- Fruit and veggie cultural tips\n",
    "    FILE_NAME = 'FruitVegCulturalItems.json'\n",
    "    print(f'Transforming \"{FILE_NAME}\"...')\n",
    "    df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "    \n",
    "    df['source'] = 'ucipm'      \n",
    "    \n",
    "    df.rename(columns = {'name': 'title'}, inplace = True)\n",
    "\n",
    "    df['images'] = df['images'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'images', 'caption'), axis = 1)\n",
    "\n",
    "    df['tips_table'] = df['tips_table'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: transform_table(r), axis = 1)\n",
    "\n",
    "    for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)]:\n",
    "        df[col] = df[col].apply(clean)\n",
    "    \n",
    "    return df\n",
    "\n",
    "infoFruitVegCultural().to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GarderContolsPestItems.json`\n",
    "<a id='gardercontolspestitemsjson'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infoPestControl():\n",
    "    # -------------------------------------------- Garden pest control\n",
    "    FILE_NAME = 'GardenControlsPestItems.json'\n",
    "    print(f'Transforming \"{FILE_NAME}\"...')\n",
    "    df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "    \n",
    "    df['source'] = 'ucipm'\n",
    "    \n",
    "    df.rename(columns = {'name': 'title'}, inplace = True)\n",
    "\n",
    "    df['images'] = df['images'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'images', 'caption'), axis = 1)\n",
    "\n",
    "    for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)]:\n",
    "        df[col] = df[col].apply(clean)\n",
    "    \n",
    "    return df\n",
    "\n",
    "infoPestControl().to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GardenControlsPesticideItems.json`\n",
    "<a id='gardencontrolspesticideitemsjson'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def infoPesticideControl():\n",
    "    # -------------------------------------------- Garden pesticide control\n",
    "    FILE_NAME = 'GardenControlsPesticideItems.json'\n",
    "    print(f'Transforming \"{FILE_NAME}\"...')\n",
    "    df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "    \n",
    "    df['source'] = 'ucipm'\n",
    "    \n",
    "    df['title'] = df[['active_ingredient', 'pesticide_type']].agg(' - '.join, axis=1)\n",
    "    df.drop(['active_ingredient', 'pesticide_type'], axis=1, inplace=True)\n",
    "\n",
    "    df.apply(lambda r: transform_pesticide(r), axis = 1)\n",
    "\n",
    "    for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)]:\n",
    "        df[col] = df[col].apply(clean)\n",
    "\n",
    "    return df\n",
    "\n",
    "infoPesticideControl().to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PestNotes.json`\n",
    "<a id='pestnotesjson'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pestsNotes():\n",
    "    # -------------------------------------------- Pests IPM\n",
    "    FILE_NAME = 'PestNotes.json'\n",
    "    print(f'Transforming \"{FILE_NAME}\"...')\n",
    "    df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "    \n",
    "    df['source'] = 'ucipm'\n",
    "    \n",
    "    df.rename(columns = {\n",
    "        'urlPestNote'           : 'url'         ,\n",
    "        'name'                  : 'title'       ,\n",
    "        'descriptionPestNote'   : 'description' ,\n",
    "        'lifecyclePestNote'     : 'lifecycle'   ,\n",
    "        'damagePestNote'        : 'damage'      ,\n",
    "        'managementPestNote'    : 'management'  ,\n",
    "        'imagePestNote'         : 'image'       ,\n",
    "    }, inplace = True)\n",
    "\n",
    "    df.drop('tablePestNote', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    df['image'] = df['image'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'image', 'caption'), axis = 1)\n",
    "\n",
    "    for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)]:\n",
    "        df[col] = df[col].apply(clean)\n",
    "    \n",
    "    return df\n",
    "\n",
    "pestsNotes().to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `QuickTips.json`\n",
    "<a id='quicktipsjson'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pestsQuickTips():\n",
    "    # -------------------------------------------- Quick tips on pests\n",
    "    FILE_NAME = 'QuickTips.json'\n",
    "    print(f'Transforming \"{FILE_NAME}\"...')\n",
    "    df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "    \n",
    "    df['source'        ] = 'ucipm'\n",
    "    \n",
    "    df.rename(columns = {\n",
    "        'urlQuickTip'           : 'url'     ,\n",
    "        'name'                  : 'title'   ,\n",
    "        'contentQuickTips'      : 'content' ,\n",
    "        'imageQuickTips'        : 'image'   ,\n",
    "    }, inplace = True)\n",
    "\n",
    "\n",
    "    df['image'] = df['image'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'image', 'caption'), axis = 1)\n",
    "\n",
    "    for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)]:\n",
    "        df[col] = df[col].apply(clean)\n",
    "    \n",
    "    return df\n",
    "\n",
    "pestsQuickTips().to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Videos.json`\n",
    "<a id='videosjson'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pestsVideos():\n",
    "    # -------------------------------------------- Videos of UC IPM YouTube data\n",
    "    FILE_NAME = 'Videos.json'\n",
    "    print(f'Transforming \"{FILE_NAME}\"...')\n",
    "    df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "    \n",
    "    df['source'] = 'ucipm'\n",
    "\n",
    "    for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)]:\n",
    "        df[col] = df[col].apply(clean)\n",
    "    \n",
    "    return df\n",
    "\n",
    "pestsVideos().to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `WeedIdItems.json`\n",
    "<a id='weediditemsjson'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pestsWeed():\n",
    "    # -------------------------------------------- Weed related pests\n",
    "    FILE_NAME = 'WeedIdItems.json'\n",
    "    print(f'Transforming \"{FILE_NAME}\"...')\n",
    "    df = pd.read_json(Path.joinpath(_PATH, FILE_NAME))\n",
    "    \n",
    "    df['source'] = 'ucipm'\n",
    "    \n",
    "    df.rename(columns = {'name'  : 'title',}, inplace = True)\n",
    "\n",
    "    df['images'] = df['images'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    df.apply(lambda r: update_key(r, 'images', 'caption'), axis = 1)\n",
    "\n",
    "    for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)]:\n",
    "        df[col] = df[col].apply(clean)\n",
    "\n",
    "    return df\n",
    "\n",
    "pestsWeed().to_json('test.json', orient='records')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging and transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('FruitVegCulturalItems.json', infoFruitVegCultural()),\n",
    "    ('GardenControlPestItems.json', infoPestControl()),\n",
    "    ('GardenControlsPesticideItems.json', infoPesticideControl()),\n",
    "    ('PestNotes.json', pestsNotes()),\n",
    "    ('QuickTips.json', pestsQuickTips()),\n",
    "    ('Videos.json', pestsVideos()),\n",
    "    ('WeedIdItems.json', pestsWeed())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE      = 1\n",
    "ROLLING_SIZE    = 3\n",
    "\n",
    "for file_name, df in data:\n",
    "\n",
    "    print(f'File name - \"{file_name}\"...')\n",
    "\n",
    "    cols = [col for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)] if col not in ['url', 'source']]\n",
    "    list_cols = df.columns[df.applymap(lambda x: isinstance(x, list)).all(0)].values\n",
    "\n",
    "    print(f'Transforming columns - text {cols} and links {list_cols}].')\n",
    "\n",
    "    from spacy.lang.en import English \n",
    "\n",
    "    nlp = English()\n",
    "    nlp.add_pipe('sentencizer')\n",
    "\n",
    "    print(f'STARTING TRANSFORMING - CHUNK_SIZE - {CHUNK_SIZE}, ROLLING_SIZE = {ROLLING_SIZE}')\n",
    "    c_items = []\n",
    "    for i, r in df.iterrows():\n",
    "        r_texts = []\n",
    "\n",
    "        n_sentences = 0\n",
    "        for c in cols:\n",
    "            t = r[c]\n",
    "            \n",
    "            doc = nlp(t)\n",
    "            \n",
    "            ts = [sent for sent in doc.sents]\n",
    "            if len(ts) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                chunks, chunk_size, roll_size = len(ts), CHUNK_SIZE, ROLLING_SIZE\n",
    "                ts = [ts[i1:i1+chunk_size+(roll_size - 1)] for i1 in range(0, chunks - (roll_size - 1), chunk_size)]\n",
    "                ts = [{'text': ' '.join([l2.text for l2 in l1]), 'name': c + '_' + str(i1), 'start': l1[0].start_char, 'end': l1[-1].end_char} for i1, l1 in enumerate(ts)]\n",
    "            \n",
    "            n_sentences += len(ts)\n",
    "            r_texts.extend(ts)\n",
    "        \n",
    "        links_exits = False\n",
    "        for lc in list_cols:\n",
    "            ts = [i1['title'] for i1 in r[lc]]\n",
    "            for i1, v in enumerate(ts):\n",
    "                r_texts.append({'text': v, 'name': lc + '.' + 'title' + '_' + str(i1), 'start': 0, 'end': -1})\n",
    "        \n",
    "        \n",
    "        c_items.append(r_texts)\n",
    "        # break\n",
    "        if (i+1) % 500 == 0:\n",
    "            print(f'Finished transforming of {i+1} rows of dataframe')\n",
    "\n",
    "    print(f'Finished transforming of {i+1} rows of dataframe')\n",
    "    print(f'FINISHED TRANSFORMING')\n",
    "\n",
    "    texts = [r1['text'] for r in c_items for r1 in r]\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "\n",
    "    print(f'STARTING EMBEDDING - BATCH_SIZE = {BATCH_SIZE}')\n",
    "    df['vectors'] = np.empty((len(df), 0)).tolist()\n",
    "\n",
    "    # TF HUB model\n",
    "    # vectors   = config.embed(texts_modified).numpy().tolist()\n",
    "        \n",
    "    # Sentence Encoder model        \n",
    "    vectors = config.embed.encode(\n",
    "        sentences           = texts     ,\n",
    "        batch_size          = BATCH_SIZE,\n",
    "        show_progress_bar   = True\n",
    "    ).tolist()\n",
    "\n",
    "    index = 0\n",
    "    for i, r in enumerate(c_items):\n",
    "        for i1, r1 in enumerate(r):\n",
    "            r1['vector'] = vectors[index]\n",
    "            r1.pop('text')\n",
    "            index += 1\n",
    "\n",
    "    print(f'FINISHED EMBEDDING')\n",
    "\n",
    "    df['vectors'] = c_items\n",
    "    print(f'The number of vectors to be ingested: {len([r1[\"vector\"] for r in df[\"vectors\"] for r1 in r])}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting data into ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Final mapping__\n",
    "```json\n",
    "{\n",
    "    # mandatory fields\n",
    "    \"url\"       : \"url\",                            # Main URL\n",
    "    \"source\"    : \"ucipm|aekb|okstate|orstate\",     # Source Dataset\n",
    "    \"title\"     : \"title\",                          # Title of data item\n",
    "    ...\n",
    "    # other fields\n",
    "    ...\n",
    "    \"vectors\"   : {\n",
    "        \"name\"  : \"field_name_and_index\",           # Name of the field\n",
    "        \"start\" : \"number\",                         # Start index within text\n",
    "        \"end\"   : \"number\",                         # End index within text\n",
    "        \"vector\": \"dense_vector\",                   # Embedding vector\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# increase the timeout if necessary\n",
    "es_client = Elasticsearch([config.es_host], http_auth=(config.es_username, config.es_password), timeout = 20)\n",
    "\n",
    "# play with chunk size parameter for timed out problem\n",
    "for file_name, df in data:\n",
    "    print(f'Ingesting \"{file_name}\"...')\n",
    "    final_json = df.to_dict(orient = 'records')\n",
    "    deque(parallel_bulk(es_client, actions = final_json, index = config.es_combined_index, max_chunk_bytes = 5 * 1024 * 1024), maxlen = 0)\n",
    "\n",
    "es_client.indices.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AskExtension Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PATH = Path('../data/askextension_kb/')\n",
    "DATA_FILE_NAMES = sorted(_PATH.iterdir())\n",
    "\n",
    "print(f'List of files:\\n{[data_file.name for data_file in DATA_FILE_NAMES]}')\n",
    "\n",
    "with open(DATA_FILE_NAMES[0]) as f:\n",
    "    f = json.load(f)\n",
    "    print(json.dumps(f[0], indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of files should be as following:\n",
    "```python\n",
    "['2012-2013.json', '2014-2015.json', '2016-2017.json', '2018-2019.json', '2020-1.json', '2020-2.json', '2021-1.json', '2021-2.json']\n",
    "```\n",
    "\n",
    "__NB__: We will only using tickets from California state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines the data files into one and returns it.\n",
    "df = pd.DataFrame()\n",
    "for f in DATA_FILE_NAMES:\n",
    "    df = pd.concat([df, pd.read_json(f)], ignore_index = True, axis = 0)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_answer(answer_dict):\n",
    "    '''\n",
    "    Convert answer field from a dictionary to a list.\n",
    "    '''\n",
    "    answers = [{}] * len(answer_dict)\n",
    "    \n",
    "    for k, v in answer_dict.items():\n",
    "        # clean the response up\n",
    "        v = {\n",
    "            'response' : clean(v['response']),\n",
    "        }\n",
    "        answers[int(k) - 1] = v\n",
    "    \n",
    "    return answers\n",
    "\n",
    "def transform_title(title):\n",
    "    '''\n",
    "    Remove question ID from title, and append '.' in the end\n",
    "    if no punctuation was detected.\n",
    "\n",
    "    Example with '#' - 437259\n",
    "    Example with '...' - 437264\n",
    "    '''\n",
    "    title = ''.join(title.split('#')[:-1]).strip().strip('...')\n",
    "    \n",
    "    # add a '.' if it does not yet end with a punctuation\n",
    "    title = title if (title and title[-1] in pn) else title + '.'\n",
    "    \n",
    "    return title\n",
    "\n",
    "def merge_title_question(df):\n",
    "    '''\n",
    "    Create new column from questions and title,\n",
    "    but only if it is not already exactly in the question.\n",
    "    '''\n",
    "    titles      = df['title'    ].tolist()\n",
    "    questions   = df['question' ].tolist()\n",
    "    \n",
    "    tqs = [\n",
    "        question\n",
    "        if (title and question.startswith(title[:-1]))\n",
    "        else title + \" \" + question\n",
    "        for (title, question) in zip(titles, questions)\n",
    "    ]\n",
    "\n",
    "    return tqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "\n",
    "from string import punctuation as pn\n",
    "\n",
    "# Modify STATE_FILTER and MIN_WORD_COUNT variables accordingly\n",
    "STATE_FILTER    = ['California', 'Oklahoma', 'Oregon']\n",
    "MIN_WORD_COUNT  = 3\n",
    "\n",
    "ASKEXTENSION_QUESTION_URL = 'https://ask2.extension.org/kb/faq.php?id='\n",
    "\n",
    "df['source'] = 'ae-kb'\n",
    "\n",
    "# Convert 'faq-id' to str type\n",
    "df['faq-id'] = df['faq-id'].astype(str)\n",
    "\n",
    "# Leave tickets from California state\n",
    "df = df[df['state'].isin(STATE_FILTER)]\n",
    "\n",
    "# Add the URL and leave blank URL for questions with no ID\n",
    "df['url'] = [\n",
    "    f\"{ASKEXTENSION_QUESTION_URL}{ticket_no}\" if len(ticket_no) == 6 else \"\"\n",
    "    for ticket_no in df['title'].str.split('#').str[-1]\n",
    "]\n",
    "\n",
    "# Add the ticket number from title and leave blank for questions without\n",
    "df['ticket-no'] = [\n",
    "    ticket_no if len(ticket_no) == 6 else \"\"\n",
    "    for ticket_no in df['title'].str.split('#').str[-1]\n",
    "]\n",
    "\n",
    "df.rename(columns = {'faq-id': 'faq_id', 'ticket-no': 'ticket_no'}, inplace = True)\n",
    "\n",
    "# Transform answer\n",
    "df['answers'] = df['answer'].apply(transform_answer)\n",
    "\n",
    "# Strip all spaces and remove non-ascii characters from text fields\n",
    "for column in ['state', 'title', 'question']:\n",
    "    df[column] = df[column].apply(clean)\n",
    "\n",
    "# Clean ID and '...' from title, and append punctuation if not present\n",
    "df['title'] = df['title'].apply(transform_title)\n",
    "\n",
    "# Create new column from `title` and `question`, or only question\n",
    "# if title is exactly the question     \n",
    "df['question'] = merge_title_question(df)\n",
    "    \n",
    "# Remove questions with small number words in title-question\n",
    "if MIN_WORD_COUNT:\n",
    "    df = df[df['question'].str.split().str.len() > MIN_WORD_COUNT]\n",
    "\n",
    "df = df.loc[:, ['source', 'url', 'state', 'title', 'question', 'answers']]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('state').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding text fields into vectors and stripping text fields for saving into ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE      = 1\n",
    "ROLLING_SIZE    = 3\n",
    "\n",
    "\n",
    "print(f'AE KB data sources...')\n",
    "\n",
    "cols = [col for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)] if col not in ['url', 'source', 'state']]\n",
    "list_cols = df.columns[df.applymap(lambda x: isinstance(x, list)).all(0)].values\n",
    "\n",
    "print(f'Transforming columns - text {cols} and links {list_cols}].')\n",
    "\n",
    "from spacy.lang.en import English \n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "print(f'STARTING TRANSFORMING - CHUNK_SIZE - {CHUNK_SIZE}, ROLLING_SIZE = {ROLLING_SIZE}')\n",
    "c_items = []\n",
    "for i, r in df.iterrows():\n",
    "    r_texts = []\n",
    "\n",
    "    n_sentences = 0\n",
    "    for c in cols:\n",
    "        t = r[c]\n",
    "        \n",
    "        doc = nlp(t)\n",
    "        \n",
    "        ts = [sent for sent in doc.sents]\n",
    "        if len(ts) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            chunks, chunk_size, roll_size = len(ts), CHUNK_SIZE, ROLLING_SIZE\n",
    "            ts = [ts[i1:i1+chunk_size+(roll_size - 1)] for i1 in range(0, chunks - (roll_size - 1), chunk_size)]\n",
    "            ts = [{'text': ' '.join([l2.text for l2 in l1]), 'name': c + '_' + str(i1), 'start': l1[0].start_char, 'end': l1[-1].end_char} for i1, l1 in enumerate(ts)]\n",
    "        \n",
    "        n_sentences += len(ts)\n",
    "        r_texts.extend(ts)\n",
    "    \n",
    "    links_exits = False\n",
    "    for lc in list_cols:\n",
    "        ts = [i1['response'] for i1 in r[lc]]\n",
    "        for i1, v in enumerate(ts):\n",
    "            r_texts.append({'text': v, 'name': lc + '.' + 'response' + '_' + str(i1), 'start': 0, 'end': -1})\n",
    "    \n",
    "    \n",
    "    c_items.append(r_texts)\n",
    "    # break\n",
    "    if (i+1) % 500 == 0:\n",
    "        print(f'Finished transforming of {i+1} rows of dataframe')\n",
    "\n",
    "print(f'Finished transforming of {i+1} rows of dataframe')\n",
    "print(f'FINISHED TRANSFORMING')\n",
    "\n",
    "texts = [r1['text'] for r in c_items for r1 in r]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(f'STARTING EMBEDDING - BATCH_SIZE = {BATCH_SIZE}')\n",
    "df['vectors'] = np.empty((len(df), 0)).tolist()\n",
    "\n",
    "# TF HUB model\n",
    "# vectors   = config.embed(texts_modified).numpy().tolist()\n",
    "    \n",
    "# Sentence Encoder model        \n",
    "vectors = config.embed.encode(\n",
    "    sentences           = texts     ,\n",
    "    batch_size          = BATCH_SIZE,\n",
    "    show_progress_bar   = True\n",
    ").tolist()\n",
    "\n",
    "index = 0\n",
    "for i, r in enumerate(c_items):\n",
    "    for i1, r1 in enumerate(r):\n",
    "        r1['vector'] = vectors[index]\n",
    "        r1.pop('text')\n",
    "        index += 1\n",
    "\n",
    "print(f'FINISHED EMBEDDING')\n",
    "\n",
    "df['vectors'] = c_items\n",
    "print(f'The number of vectors to be ingested: {len([r1[\"vector\"] for r in df[\"vectors\"] for r1 in r])}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for invalid links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ucipm = 0\n",
    "count_askextension = 0\n",
    "for i, r in finalDf.iterrows():\n",
    "    url = r['url']\n",
    "    if len(r['url']) < 10:\n",
    "        print(f'Source with no main link at row {i} of data frame, main link - {url}.')\n",
    "    links = r['links']\n",
    "    no_link = False\n",
    "    show_main_url = False\n",
    "    for l in links:\n",
    "        if len(l['src']) < 10:\n",
    "            no_link = True\n",
    "            if not show_main_url:\n",
    "                show_main_url = True\n",
    "                print(f'Links at {url}')\n",
    "            print(l)\n",
    "    if no_link:\n",
    "        if r['source'] == 'ae_kb':\n",
    "            count_askextension += 1\n",
    "        else:\n",
    "            count_ucipm += 1\n",
    "        \n",
    "\n",
    "print(f'Number of sources from AskExtension with no link urls - {count_askextension}')\n",
    "print(f'Number of sources from UC IPM with no link urls - {count_ucipm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [r1['text'] for r in c_items for r1 in r]\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(f'STARTING EMBEDDING')\n",
    "\n",
    "finalDf['vectors'] = np.empty((len(finalDf), 0)).tolist()\n",
    "\n",
    "# TF HUB model\n",
    "# vectors   = config.embed(texts_modified).numpy().tolist()\n",
    "    \n",
    "# Sentence Encoder model        \n",
    "vectors = config.embed.encode(\n",
    "    sentences           = texts_modified,\n",
    "    batch_size          = BATCH_SIZE    ,\n",
    "    show_progress_bar   = True\n",
    ").tolist()\n",
    "\n",
    "index = 0\n",
    "for i, r in enumerate(c_items):\n",
    "    for i1, r1 in enumerate(r):\n",
    "        r1['vector'] = vectors[index]\n",
    "        r1.pop('text')\n",
    "        index += 1\n",
    "\n",
    "print(f'FINISHED EMBEDDING')\n",
    "\n",
    "finalDf['vectors'] = c_items\n",
    "print(f'The number of vectors to be ingested: {len([r1[\"vector\"] for r in finalDf[\"vectors\"] for r1 in r])}')        \n",
    "finalDf.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting data into ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different embedding sizes depending on the models\n",
    "# VECTOR_SIZE = 384\n",
    "# VECTOR_SIZE = 512\n",
    "VECTOR_SIZE = 768\n",
    "\n",
    "mapping  = {\n",
    "    \"settings\": {\"number_of_shards\": 2, \"number_of_replicas\": 1},\n",
    "    \"mappings\": {\n",
    "        \"dynamic\"   : \"false\",\n",
    "        \"_source\"   : {\"enabled\": \"true\"},\n",
    "        \"properties\": {\n",
    "            \"source\"        : {\"type\": \"keyword\", \"index\": \"true\" , \"ignore_above\": 32766},\n",
    "            \"url\"           : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": 32766},\n",
    "\n",
    "            \"title\"         : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": 32766},\n",
    "            \"description\"   : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": 32766},\n",
    "            \"identification\": {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": 32766},\n",
    "            \"development\"   : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": 32766},\n",
    "            \"damage\"        : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": 32766},\n",
    "            \"management\"    : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": 32766},\n",
    "            \"vectors\"       : {\n",
    "                \"type\"      : \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"vector\": {\n",
    "                        \"type\": \"dense_vector\", \n",
    "                        \"dims\": VECTOR_SIZE\n",
    "                    },\n",
    "                    \"name\"  : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": 32766},\n",
    "                    \"start\" : {\"type\": \"integer\"                                         },\n",
    "                    \"end\"   : {\"type\": \"integer\"                                         },\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            \"links\"         : {\n",
    "                \"type\"      : \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"type\"  : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": 32766},\n",
    "                    \"src\"   : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": 32766},\n",
    "                    \"link\"  : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": 32766},\n",
    "                    \"title\" : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": 32766}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "final_json = finalDf.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "# increase the timeout if necessary\n",
    "es_client = Elasticsearch([config.es_host], http_auth=(config.es_username, config.es_password), timeout = 20)\n",
    "\n",
    "es_client.indices.delete(\n",
    "    index   = config.es_combined_index, \n",
    "    ignore  = 404)\n",
    "es_client.indices.create(\n",
    "    index       = config.es_combined_index  , \n",
    "    settings    = mapping['settings']       , \n",
    "    mappings    = mapping['mappings']       )\n",
    "# play with chunk size parameter for timed out problem\n",
    "deque(parallel_bulk(es_client, actions = final_json, index = config.es_combined_index, max_chunk_bytes = 5 * 1024 * 1024), maxlen = 0)\n",
    "\n",
    "es_client.indices.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('es-data-ingestion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "607e550c5f7577ecefb8f11c45030a36424a53b9c08a45019262888187990ca7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
