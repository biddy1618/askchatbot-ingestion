{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from spacy.lang.en import English\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_USERNAME = 'elastic'\n",
    "ES_PASSWORD = 'changeme'\n",
    "ES_INDEX = 'test2'\n",
    "ES_HOST = 'http://localhost:9200/'\n",
    "# ES_HOST = 'https://dev.es.chat.ask.eduworks.com/'\n",
    "# ES_HOST = 'https://qa.es.chat.ask.eduworks.com/'\n",
    "EMBED_CACHE_URL = '/var/tmp/models'\n",
    "# MODEL_URL = 'all-distilroberta-v1'\n",
    "MODEL_URL = \"JeffEduworks/generalized_chatbot_model\"\n",
    "AUTH_TOKEN = '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'\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    model_name_or_path=MODEL_URL,\n",
    "    use_auth_token=AUTH_TOKEN,\n",
    "    cache_folder= EMBED_CACHE_URL,\n",
    "    device='cuda'            \n",
    ")\n",
    "MAX_SEQ_SIZE = model.max_seq_length\n",
    "VECTOR_SIZE = model[1].word_embedding_dimension\n",
    "BATCH_SIZE = 64\n",
    "MAX_STRING_SIZE = 32766\n",
    "MAPPING  = {\n",
    "    \"settings\": {\"number_of_shards\": 2, \"number_of_replicas\": 1},\n",
    "    \"mappings\": {\n",
    "        \"dynamic\"       : \"false\"   ,\n",
    "        \"date_detection\": \"false\"   ,\n",
    "        \"_source\"   : {\"enabled\": \"true\"},\n",
    "        \"properties\": {\n",
    "            \"source\"        : {\"type\": \"keyword\", \"index\": \"true\" , \"ignore_above\": MAX_STRING_SIZE},\n",
    "            \"url\"           : {\"type\": \"keyword\", \"index\": \"true\" , \"ignore_above\": MAX_STRING_SIZE},\n",
    "\n",
    "            \"title\"         : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": MAX_STRING_SIZE},\n",
    "            \"images\"        : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": MAX_STRING_SIZE},\n",
    "            \"vectors\"       : {\n",
    "                \"type\"      : \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"vector\": {\n",
    "                        \"type\": \"dense_vector\", \n",
    "                        \"dims\": VECTOR_SIZE\n",
    "                    },\n",
    "                    \"field\" : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": MAX_STRING_SIZE},\n",
    "                    \"name\"  : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": MAX_STRING_SIZE},\n",
    "                    \"im_src\": {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": MAX_STRING_SIZE},\n",
    "                    \"text\"  : {\"type\": \"keyword\", \"index\": \"false\", \"ignore_above\": MAX_STRING_SIZE},\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(texts, max_seq_length):\n",
    "    '''\n",
    "    Chunk the text into fragments no longer than maximum sequence length.\n",
    "    '''\n",
    "    texts_new = []\n",
    "\n",
    "    for item in texts:\n",
    "        text, field, name, im_src = item['text'], item['field'], item['name'], item['im_src']\n",
    "        doc = nlp(text)\n",
    "        sents = [sent for sent in doc.sents]\n",
    "        \n",
    "        if len(text) == 0:\n",
    "            print(item)\n",
    "        start, end = 0, 1\n",
    "        while end != len(sents):\n",
    "            if start == end:\n",
    "                end +=1\n",
    "            elif len(' '.join([sent.text for sent in sents[start:end+1]])) > max_seq_length:\n",
    "                texts_new.append({\n",
    "                    'text': ' '.join([sent.text for sent in sents[start:end]]),\n",
    "                    'field': field,\n",
    "                    'name': name,\n",
    "                    'im_src': im_src\n",
    "                })\n",
    "                start += 1\n",
    "            else:\n",
    "                end += 1\n",
    "\n",
    "        texts_new.append({\n",
    "            'text': ' '.join([sent.text for sent in sents[start:end]]),\n",
    "            'field': field,\n",
    "            'name': name,\n",
    "            'im_src': im_src\n",
    "        })\n",
    "\n",
    "    return texts_new\n",
    "\n",
    "\n",
    "def get_transformed_data():\n",
    "    DATA_PATH = Path.joinpath(Path().absolute().parents[0], 'data/transformed')\n",
    "\n",
    "    if not DATA_PATH.is_dir():\n",
    "        raise FileNotFoundError(\n",
    "            (\n",
    "                'Folder \\'/data/transformed\\' not available.'\n",
    "                ' Data folder is empty or not created. Make sure to create data folder.'\n",
    "                ' Follow the instruction in the \\'README-es-ingesting-data.md\\' file.'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    DATA_FILE_NAMES = sorted(DATA_PATH.iterdir())\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    for f in DATA_FILE_NAMES:\n",
    "        df = pd.concat([df, pd.read_json(f)], ignore_index = True, axis = 0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_transformed_data()\n",
    "\n",
    "print(f'STARTING TRANSFORMING...')\n",
    "df_texts = []\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    texts = row['texts']\n",
    "    df_texts.append(get_chunks(texts, MAX_SEQ_SIZE))\n",
    "    if (i+1) % 500 == 0:\n",
    "        print(f'Finished transforming of {i+1} rows of dataframe')\n",
    "    \n",
    "    \n",
    "print(f'Finished transforming of {i+1} rows of dataframe')\n",
    "print(f'FINISHED TRANSFORMING')\n",
    "texts = [item['text'] for row in df_texts for item in row]\n",
    "print(f'STARTING EMBEDDING - BATCH_SIZE = {BATCH_SIZE}')\n",
    "print(f'Number of texts to be embedded = {len(texts)}')\n",
    "# Sentence Encoder model        \n",
    "vectors = model.encode(\n",
    "    sentences           = texts     ,\n",
    "    batch_size          = BATCH_SIZE,\n",
    "    show_progress_bar   = True\n",
    ").tolist()\n",
    "\n",
    "index = 0\n",
    "for i, row in enumerate(df_texts):\n",
    "    for i1, item in enumerate(row):\n",
    "        item['vector'] = vectors[index]\n",
    "        item['field'] = item['field'] + str(i1)\n",
    "        assert texts[index] == item['text']\n",
    "        index += 1\n",
    "\n",
    "print(f'FINISHED EMBEDDING')\n",
    "df['texts'] = df_texts\n",
    "print(f'The number of vectors to be ingested: {len([item[\"vector\"] for row in df[\"texts\"] for item in row])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given the data ingest it into ES instance.\n",
    "'''\n",
    "# increase the timeout if necessary\n",
    "es_client = Elasticsearch([ES_HOST], http_auth=(ES_USERNAME, ES_PASSWORD), timeout=20)\n",
    "es_client.indices.delete(index=ES_INDEX, ignore=404)\n",
    "es_client.indices.create(index=ES_INDEX, settings=MAPPING['settings'], mappings=MAPPING['mappings'])\n",
    "# play with chunk size parameter for timed out problem\n",
    "final_json = df.to_dict(orient='records')\n",
    "deque(parallel_bulk(es_client, actions=final_json, index=ES_INDEX, max_chunk_bytes=5*1024*1024), maxlen=0)\n",
    "es_client.indices.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('es-data-ingestion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "607e550c5f7577ecefb8f11c45030a36424a53b9c08a45019262888187990ca7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
