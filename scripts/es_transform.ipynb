{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from string import punctuation as pn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UC IPM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UC IPM helper functions\n",
    "def clean(text):\n",
    "    '''\n",
    "    Fix encodings and remove escape and redundant whitespace characters from text.\n",
    "    '''\n",
    "    text = text.encode('ascii', 'ignore').decode()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = html.unescape(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def transform_data(df, list_fields, list_text_fields, image_fields):\n",
    "    '''\n",
    "    Given data transform the data into the required format. Only applicable for UC IPM data\n",
    "    '''\n",
    "    def get_text_from_fields(row, fields):\n",
    "        '''\n",
    "        Get text from text fields.\n",
    "        '''\n",
    "        row_items = []\n",
    "\n",
    "        for field in fields:\n",
    "            text = clean(row[field])\n",
    "            if len(text) == 0:\n",
    "                continue\n",
    "            row_items.append({\n",
    "                'text': text,\n",
    "                'field': field,\n",
    "                'name': field.replace('_', ' ').capitalize(),\n",
    "                'im_src': ''\n",
    "            })\n",
    "\n",
    "        return row_items\n",
    "\n",
    "\n",
    "    def get_text_from_list_field(row, field, subfield, title=False, im_src=None):\n",
    "        '''\n",
    "        Get text from list fields.\n",
    "        '''\n",
    "        row_items = []\n",
    "        \n",
    "        for item in row[field]:\n",
    "            text = clean(item[subfield])\n",
    "            if len(text) == 0:\n",
    "                continue\n",
    "\n",
    "            if title:\n",
    "                text = row['title'] + ' - ' + text\n",
    "            else: \n",
    "                text = text\n",
    "\n",
    "            if im_src and len(item[im_src]) > 0:\n",
    "                src = item[im_src]\n",
    "            else: \n",
    "                src = ''\n",
    "            \n",
    "            row_items.append({\n",
    "                'text': text,\n",
    "                'field': field,\n",
    "                'name': field.replace('_', ' ').capitalize(),\n",
    "                'im_src': src\n",
    "            })\n",
    "            \n",
    "        return row_items\n",
    "\n",
    "\n",
    "    def get_images(row, field, im_src):\n",
    "        '''\n",
    "        Get images from list image fields.\n",
    "        '''\n",
    "        row_images = []\n",
    "        \n",
    "        for item in row[field]:\n",
    "            if len(item[im_src]) > 0:\n",
    "                row_images.append(item[im_src])\n",
    "        \n",
    "        return row_images\n",
    "\n",
    "    for field in list_fields:\n",
    "        df[field] = df[field].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    \n",
    "    cols = [col for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)] if col not in ['url']]\n",
    "    texts = []\n",
    "    images = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        row_texts = []\n",
    "        row_images = []\n",
    "        row_texts.extend(get_text_from_fields(row, cols))\n",
    "        \n",
    "        for field, subfield, concat_title, im_src in list_text_fields:\n",
    "            row_texts.extend(get_text_from_list_field(row, field, subfield, title=concat_title, im_src=im_src))\n",
    "\n",
    "        for field, subfield in image_fields:\n",
    "            row_images.extend(get_images(row, field, subfield))\n",
    "        \n",
    "        texts.append(row_texts)\n",
    "        images.append(row_images)\n",
    "\n",
    "    df['texts'] = texts\n",
    "    df['images'] = images\n",
    "    df['title'] = df['title'].apply(clean).replace('', 'No title')\n",
    "    df = df.loc[:, ['url', 'title', 'texts', 'images']]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_table(row):\n",
    "    '''\n",
    "    Rename the 'tips_table' key values to title with title and header concatenation (inplace).\n",
    "    '''\n",
    "    if len(row['tips_table']) > 0:\n",
    "        items = row['tips_table']\n",
    "        assert 'header' in items[0] \n",
    "        header_title = row['title'] + ' - ' + items[0]['header']\n",
    "        row['tips_table'] = header_title\n",
    "    else:\n",
    "        row['tips_table'] = ''\n",
    "\n",
    "\n",
    "def transform_pesticide(row):\n",
    "    '''\n",
    "    Merge pesticide subfield into main field - information (inplace).\n",
    "    '''\n",
    "    information = row['information'][0]\n",
    "    texts = []\n",
    "\n",
    "    for k, v in information.items():\n",
    "        texts.append(k.replace('_', ' ').capitalize() + ': ' + v + '. ')\n",
    "    \n",
    "    row['information'] = '. '.join(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UC IPM December 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 1000\n",
    "'''\n",
    "UC IPM December 2021 data transformation.\n",
    "\n",
    "Data is fetched from 'data/uc-ipm/scrape_cleaned_Dec2021' folder.\n",
    "The transformed data is saved in 'data/transformed/ucipm-Dec2021.json' file  \n",
    "'''\n",
    "DATA_PATH = Path.joinpath(Path().absolute().parents[0], 'data/uc-ipm/scrape_cleaned_Dec2021')\n",
    "SAVE_FILE = Path.joinpath(Path().absolute().parents[0], 'data/transformed/ucipm-Dec2021.json')\n",
    "SAVE_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not DATA_PATH.is_dir():\n",
    "    raise FileNotFoundError(\n",
    "        (\n",
    "            'Folder \\'/data/uc-ipm/scrape_cleaned_Dec2021\\' not available.'\n",
    "            ' Data folder is empty or not created. Make sure to create data folder.'\n",
    "            ' Follow the instruction in the \\'README-es-ingesting-data.md\\' file.'\n",
    "        )\n",
    "    )\n",
    "\n",
    "DATA_FILE_NAMES = {\n",
    "    'exoticPests.json',\n",
    "    'fruitItems_new.json',\n",
    "    'fruitVeggieEnvironItems_new.json',\n",
    "    'pestDiseaseItems_new.json',\n",
    "    'plantFlowerItems.json',\n",
    "    'turfPests.json',\n",
    "    'veggieItems_new.json',\n",
    "    'weedItems.json'\n",
    "}\n",
    "\n",
    "try:\n",
    "    assert set(data_file.name for data_file in DATA_PATH.iterdir()) == DATA_FILE_NAMES\n",
    "except AssertionError:\n",
    "    raise FileNotFoundError(\n",
    "        (\n",
    "            'Data folder \\'scrape_cleaned_dec2021\\' doesn\\' contain all the files.'\n",
    "            ' Please check the commit hash of the data source and make sure it'\n",
    "            ' corresponds to the one in the \\'README-es-ingesting-data.md\\' file.'\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(f'Transforming UC IPM Dec 2021 crawl...')\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "FILE_NAME = 'exoticPests.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(DATA_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title'}, inplace = True)\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images', 'related_links'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "        ('related_links', 'text', True, None),        \n",
    "    ],\n",
    "    image_fields=[('images', 'src')]\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'fruitItems_new.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(DATA_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title'}, inplace = True)\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['cultural_tips', 'pests_and_disorders'],\n",
    "    list_text_fields=[\n",
    "        ('cultural_tips', 'tip', True, None),\n",
    "        ('pests_and_disorders', 'problem', True, None),        \n",
    "    ],\n",
    "    image_fields=[]\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'fruitVeggieEnvironItems_new.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(DATA_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title',}, inplace = True)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),        \n",
    "    ],\n",
    "    image_fields=[('images', 'src')]\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'pestDiseaseItems_new.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(DATA_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title',}, inplace = True)\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),        \n",
    "    ],\n",
    "    image_fields=[('images', 'src')]\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'plantFlowerItems.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(DATA_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title',}, inplace = True)\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images', 'pests_and_disorders'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "        ('pests_and_disorders', 'problem', True, None),        \n",
    "    ],\n",
    "    image_fields=[('images', 'src')]\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'turfPests.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(DATA_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title',}, inplace = True)\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),        \n",
    "    ],\n",
    "    image_fields=[('images', 'src')]\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'veggieItems_new.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(DATA_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name'  : 'title'}, inplace = True)\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images', 'pests_and_disorders'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "        ('pests_and_disorders', 'problem', True, None),        \n",
    "    ],\n",
    "    image_fields=[('images', 'src')]\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'weedItems.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(DATA_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title',}, inplace = True)\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, None),        \n",
    "    ],\n",
    "    image_fields=[]\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "final_df['source'] = 'ucipm'\n",
    "    \n",
    "\n",
    "if limit:\n",
    "    final_df = final_df.sample(limit) if limit < final_df.shape[0] else final_df\n",
    "\n",
    "print(f'Final shape is :{final_df.shape}')\n",
    "final_df.to_json(SAVE_FILE, orient='records')\n",
    "print(f'Saved file to {SAVE_FILE}')\n",
    "\n",
    "final_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UC IPM April 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 1000\n",
    "'''\n",
    "UC IPM December 2022 data transformation.\n",
    "\n",
    "Data is fetched from 'data/uc-ipm/scrape_cleaned_Apr2022' folder.\n",
    "The transformed data is saved in 'data/transformed/ucipm-Dec2021.json' file  \n",
    "'''\n",
    "\n",
    "DATA_PATH = Path.joinpath(Path().absolute().parents[0], 'data/uc-ipm/scrape_cleaned_Apr2022')\n",
    "SAVE_FILE = Path.joinpath(Path().absolute().parents[0],'data/transformed/ucipm-Apr2022.json')\n",
    "SAVE_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not DATA_PATH.is_dir():\n",
    "    raise FileNotFoundError(\n",
    "        (\n",
    "            'Folder \\'/data/uc-ipm/scrape_cleaned_Dec2021\\' not available.'\n",
    "            ' Data folder is empty or not created. Make sure to create data folder.'\n",
    "            ' Follow the instruction in the \\'README-es-ingesting-data.md\\' file.'\n",
    "        )\n",
    "    )\n",
    "\n",
    "DATA_FILE_NAMES = {\n",
    "    'FruitVegCulturalItems.json',\n",
    "    'GardenControlsPestItems.json',\n",
    "    'GardenControlsPesticideItems.json',\n",
    "    'PestNotes.json',\n",
    "    'QuickTips.json',\n",
    "    'Videos.json',\n",
    "    'WeedIdItems.json'\n",
    "}\n",
    "\n",
    "try:\n",
    "    assert set(data_file.name for data_file in DATA_PATH.iterdir()) == DATA_FILE_NAMES\n",
    "except AssertionError:\n",
    "    raise FileNotFoundError(\n",
    "        (\n",
    "            'Data folder \\'scrape_cleaned_dec2021\\' doesn\\' contain all the files.'\n",
    "            ' Please check the commit hash of the data source and make sure it'\n",
    "            ' corresponds to the one in the \\'README-es-ingesting-data.md\\' file.'\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(f'Transforming UC IPM Apr 2022 crawl...')\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "FILE_NAME = 'FruitVegCulturalItems.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(DATA_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title'}, inplace = True)\n",
    "df['tips_table'] = df['tips_table'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "df.apply(lambda r: transform_table(r), axis = 1)\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "    ],\n",
    "    image_fields=[('images', 'src')]\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'GardenControlsPestItems.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(DATA_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name': 'title'}, inplace = True)\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "    ],\n",
    "    image_fields=[('images', 'src')]\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'GardenControlsPesticideItems.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(DATA_PATH, FILE_NAME))\n",
    "df['title'] = df[['active_ingredient', 'pesticide_type']].agg(' - '.join, axis=1)\n",
    "df.drop(['active_ingredient', 'pesticide_type'], axis=1, inplace=True)\n",
    "df.apply(lambda r: transform_pesticide(r), axis = 1)\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=[],\n",
    "    list_text_fields=[],\n",
    "    image_fields=[]\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'PestNotes.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(DATA_PATH, FILE_NAME))\n",
    "df.rename(columns = {\n",
    "    'urlPestNote'           : 'url'         ,\n",
    "    'name'                  : 'title'       ,\n",
    "    'descriptionPestNote'   : 'description' ,\n",
    "    'lifecyclePestNote'     : 'lifecycle'   ,\n",
    "    'damagePestNote'        : 'damage'      ,\n",
    "    'managementPestNote'    : 'management'  ,\n",
    "    'imagePestNote'         : 'images'      ,\n",
    "}, inplace = True)\n",
    "df.drop('tablePestNote', axis=1, inplace=True)\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "    ],\n",
    "    image_fields=[('images', 'src')]\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'QuickTips.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(DATA_PATH, FILE_NAME))\n",
    "df.rename(columns = {\n",
    "    'urlQuickTip'           : 'url'     ,\n",
    "    'name'                  : 'title'   ,\n",
    "    'contentQuickTips'      : 'content' ,\n",
    "    'imageQuickTips'        : 'images'  ,\n",
    "}, inplace = True)\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "    ],\n",
    "    image_fields=[('images', 'src')]\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'Videos.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(DATA_PATH, FILE_NAME))\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=[],\n",
    "    list_text_fields=[],\n",
    "    image_fields=[],\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "\n",
    "FILE_NAME = 'WeedIdItems.json'\n",
    "print(f'Transforming \"{FILE_NAME}\"...')\n",
    "df = pd.read_json(Path.joinpath(DATA_PATH, FILE_NAME))\n",
    "df.rename(columns = {'name'  : 'title',}, inplace = True)\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['images'],\n",
    "    list_text_fields=[\n",
    "        ('images', 'caption', True, 'src'),\n",
    "    ],\n",
    "    image_fields=[('images', 'src')]\n",
    ")\n",
    "final_df = pd.concat([final_df, df], axis=0, ignore_index=True)\n",
    "final_df['source'] = 'ucipm'\n",
    "\n",
    "if limit:\n",
    "    final_df = final_df.sample(limit) if limit < final_df.shape[0] else final_df\n",
    "\n",
    "print(f'Final shape is :{final_df.shape}')\n",
    "final_df.to_json(SAVE_FILE, orient='records')\n",
    "print(f'Saved file to {SAVE_FILE}')\n",
    "\n",
    "final_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AskExtension data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE helper functions\n",
    "def clean(text):\n",
    "    '''\n",
    "    Fix encodings and remove escape and redundant whitespace characters from text.\n",
    "    '''\n",
    "    text = text.encode('ascii', 'ignore').decode()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = html.unescape(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def transform_data(df, list_fields, list_text_fields, image_fields):\n",
    "    '''\n",
    "    Given data transform the data into the required format. Only applicable for UC IPM data\n",
    "    '''\n",
    "    def get_text_from_fields(row, fields):\n",
    "        '''\n",
    "        Get text from text fields.\n",
    "        '''\n",
    "        row_items = []\n",
    "\n",
    "        for field in fields:\n",
    "            text = clean(row[field])\n",
    "            if len(text) == 0:\n",
    "                continue\n",
    "            row_items.append({\n",
    "                'text': text,\n",
    "                'field': field,\n",
    "                'name': field.replace('_', ' ').capitalize(),\n",
    "                'im_src': ''\n",
    "            })\n",
    "\n",
    "        return row_items\n",
    "\n",
    "\n",
    "    def get_text_from_list_field(row, field, subfield, title=False, im_src=None):\n",
    "        '''\n",
    "        Get text from list fields.\n",
    "        '''\n",
    "        row_items = []\n",
    "        \n",
    "        for item in row[field]:\n",
    "            text = clean(item[subfield])\n",
    "            if len(text) == 0:\n",
    "                continue\n",
    "\n",
    "            if title:\n",
    "                text = row['title'] + ' - ' + text\n",
    "            else: \n",
    "                text = text\n",
    "\n",
    "            if im_src and len(item[im_src]) > 0:\n",
    "                src = item[im_src]\n",
    "            else: \n",
    "                src = ''\n",
    "            \n",
    "            row_items.append({\n",
    "                'text': text,\n",
    "                'field': field,\n",
    "                'name': field.replace('_', ' ').capitalize(),\n",
    "                'im_src': src\n",
    "            })\n",
    "            \n",
    "        return row_items\n",
    "\n",
    "\n",
    "    def get_images(row, field, im_src):\n",
    "        '''\n",
    "        Get images from list image fields.\n",
    "        '''\n",
    "        row_images = []\n",
    "        \n",
    "        for item in row[field]:\n",
    "            if len(item[im_src]) > 0:\n",
    "                row_images.append(item[im_src])\n",
    "        \n",
    "        return row_images\n",
    "    \n",
    "    for field in list_fields:\n",
    "        df[field] = df[field].apply(lambda d: d if isinstance(d, list) else [])\n",
    "    \n",
    "    cols = [col for col in df.columns[df.applymap(lambda x: isinstance(x, str)).all(0)] if col not in ['url']]\n",
    "    texts = []\n",
    "    images = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        row_texts = []\n",
    "        row_images = []\n",
    "        row_texts.extend(get_text_from_fields(row, cols))\n",
    "        \n",
    "        for field, subfield, concat_title, im_src in list_text_fields:\n",
    "            row_texts.extend(get_text_from_list_field(row, field, subfield, title=concat_title, im_src=im_src))\n",
    "\n",
    "        for field, subfield in image_fields:\n",
    "            row_images.extend(get_images(row, field, subfield))\n",
    "        \n",
    "        texts.append(row_texts)\n",
    "        images.append(row_images)\n",
    "\n",
    "    df['texts'] = texts\n",
    "    df['images'] = images\n",
    "    df['title'] = df['title'].apply(clean).replace('', 'No title')\n",
    "    df = df.loc[:, ['url', 'title', 'texts', 'images']]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_answer(answer_dict):\n",
    "    '''\n",
    "    Convert answer field from a dictionary to a list.\n",
    "    '''\n",
    "    answers = [{}] * len(answer_dict)\n",
    "    \n",
    "    for k, v in answer_dict.items():\n",
    "        # clean the response up\n",
    "        v = {'response': clean(v['response'])}\n",
    "        answers[int(k) - 1] = v\n",
    "    \n",
    "    return answers\n",
    "\n",
    "\n",
    "def transform_title(title):\n",
    "    '''\n",
    "    Remove question ID from title, and append '.' in the end\n",
    "    if no punctuation was detected.\n",
    "\n",
    "    Example with '#' - 437259\n",
    "    Example with '...' - 437264\n",
    "    '''\n",
    "    title = ''.join(title.split('#')[:-1]).strip().strip('...')\n",
    "    # add a '.' if it does not yet end with a punctuation\n",
    "    title = title if (title and title[-1] in pn) else title + '.'\n",
    "    \n",
    "    return title\n",
    "\n",
    "\n",
    "def merge_title_question(df):\n",
    "    '''\n",
    "    Create new column from questions and title,\n",
    "    but only if it is not already exactly in the question.\n",
    "    '''\n",
    "    titles      = df['title'    ].tolist()\n",
    "    questions   = df['question' ].tolist()\n",
    "    tqs = [\n",
    "        question\n",
    "        if (title and question.startswith(title[:-1]))\n",
    "        else title + \" \" + question\n",
    "        for (title, question) in zip(titles, questions)\n",
    "    ]\n",
    "\n",
    "    return tqs\n",
    "\n",
    "\n",
    "def download_ask_extention_data(data_path: str, start_year: int = 2006, end_year: int = 2024):\n",
    "    '''Calls OS ticket API to get all ask extension data'''\n",
    "    for i in tqdm(range(start_year, end_year), desc='Calling OS Ticket API to download AE data...'):\n",
    "        start = str(i) \n",
    "        end = str(i+1)\n",
    "        url = f'https://qa.osticket.eduworks.com/api/knowledge/{start}-01-01/{end}-01-01'\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(url, timeout=40)\n",
    "            items = r.json()\n",
    "        except requests.exceptions.Timeout: \n",
    "            print(f\"Failed to download data for year {start}\")\n",
    "            continue\n",
    "        \n",
    "        if items:\n",
    "            PATH_SAVE = Path.joinpath(data_path, f'{start_year}.json')\n",
    "            with open(PATH_SAVE, 'w') as f:\n",
    "                json.dump(items, f)\n",
    "\n",
    "\n",
    "def get_ask_extension_data(data_path: str) -> list:\n",
    "    '''Attempts to load from AE data. Though, will call os ticket API if not available'''\n",
    "    DATA_FILE_NAMES = sorted(data_path.iterdir())\n",
    "\n",
    "    if len(DATA_FILE_NAMES) == 0:\n",
    "        download_ask_extention_data()\n",
    "\n",
    "    # Combines the data files into one and returns it.\n",
    "    df = pd.DataFrame()\n",
    "    print(f'List of files:\\n{[data_file.name for data_file in DATA_FILE_NAMES]}')\n",
    "    \n",
    "    for f in DATA_FILE_NAMES:\n",
    "        df = pd.concat([df, pd.read_json(f)], ignore_index = True, axis = 0)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 1000\n",
    "\n",
    "'''\n",
    "AE KB data transformation.\n",
    "\n",
    "Data is fetched from 'data/askextension_kb' folder.\n",
    "The transformed data is saved in 'data/transformed/ae_kb.json' file.\n",
    "'''\n",
    "DATA_PATH = Path.joinpath(Path().absolute().parents[0], 'data/askextension_kb')\n",
    "SAVE_FILE = Path.joinpath(Path().absolute().parents[0], 'data/transformed/ae_kb.json')\n",
    "SAVE_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not DATA_PATH.is_dir():\n",
    "    DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = get_ask_extension_data(DATA_PATH)\n",
    "print(f'Transforming AE KB data...')\n",
    "# Modify STATE_FILTER and MIN_WORD_COUNT variables accordingly\n",
    "STATE_FILTER    = ['California', 'Oklahoma', 'Oregon']\n",
    "# STATE_FILTER    = ['California']\n",
    "MIN_WORD_COUNT  = 3\n",
    "ASKEXTENSION_QUESTION_URL = 'https://ask2.extension.org/kb/faq.php?id='\n",
    "df['source'] = 'ae-kb'\n",
    "df['faq-id'] = df['faq-id'].astype(str)\n",
    "df = df[df['state'].isin(STATE_FILTER)]\n",
    "df['url'] = [\n",
    "    f\"{ASKEXTENSION_QUESTION_URL}{ticket_no}\" if len(ticket_no) == 6 else \"\"\n",
    "    for ticket_no in df['title'].str.split('#').str[-1]\n",
    "]\n",
    "df['ticket-no'] = [\n",
    "    ticket_no if len(ticket_no) == 6 else \"\"\n",
    "    for ticket_no in df['title'].str.split('#').str[-1]\n",
    "]\n",
    "df['attachments'] = df['attachments'].apply(lambda d: d if isinstance(d, list) else [])\n",
    "df['attachments'] = df['attachments'].apply(lambda d: [{'src': link} for link in d])\n",
    "df.rename(columns = {'faq-id': 'faq_id', 'ticket-no': 'ticket_no'}, inplace = True)\n",
    "df['answers'] = df['answer'].apply(transform_answer)\n",
    "df['title'] = df['title'].apply(transform_title)\n",
    "df['question'] = merge_title_question(df)\n",
    "\n",
    "if MIN_WORD_COUNT:\n",
    "    df = df[df['question'].str.split().str.len() > MIN_WORD_COUNT]\n",
    "\n",
    "df = df.loc[:, ['source', 'url', 'title', 'question', 'answers', 'attachments']]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df = transform_data(\n",
    "    df, \n",
    "    list_fields=['answers',],\n",
    "    list_text_fields=[\n",
    "        ('answers', 'response', False, None),\n",
    "    ],\n",
    "    image_fields=[('attachments', 'src')]\n",
    ")\n",
    "df['source'] = 'ae_kb'\n",
    "\n",
    "if limit:\n",
    "    df = df.sample(limit) if limit < df.shape[0] else df\n",
    "\n",
    "print(f'Final shape is :{df.shape}')\n",
    "df.to_json(SAVE_FILE, orient='records')\n",
    "print(f'Saved file to {SAVE_FILE}')\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oklahoma and Oregon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_and_description(row, thumbnail=False):\n",
    "    '''\n",
    "    Transform the title and description fields.\n",
    "    '''\n",
    "    title, description = clean(row['title']), clean(row['description'])\n",
    "    \n",
    "    if thumbnail:\n",
    "        im_src = row['thumbnail']\n",
    "    else:\n",
    "        im_src = ''\n",
    "\n",
    "    texts = []\n",
    "    if len(title) > 0:\n",
    "        texts.append({\n",
    "            'text': title,\n",
    "            'field': 'title',\n",
    "            'name': 'Title',\n",
    "            'im_src': im_src\n",
    "        })\n",
    "\n",
    "    if len(description) > 0:\n",
    "        texts.append({\n",
    "            'text': description,\n",
    "            'field': 'description',\n",
    "            'name': 'Description',\n",
    "            'im_src': im_src\n",
    "        })\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "def get_contents_and_images(row, thumbnail=False):\n",
    "    '''\n",
    "    Transform the content field by concatenating title with header, and perform cleaning. Drop the unncessary columns.\n",
    "    '''\n",
    "    texts = []\n",
    "    images = []\n",
    "    title = clean(row['title'])\n",
    "\n",
    "    if thumbnail:\n",
    "        im_src = row['thumbnail']\n",
    "        images.append(im_src)\n",
    "    else:\n",
    "        im_src = ''\n",
    "\n",
    "    for content in row['content']:\n",
    "        item = {}\n",
    "        header = clean(content['header'])  \n",
    "        if len(header) > 0:\n",
    "            if len(title) > 0 and header != 'Introduction-w/o-header':\n",
    "                header = clean(title + ' - ' + header)\n",
    "            elif len(title) > 0 and header == 'Introduction-w/o-header':\n",
    "                header = clean(title)\n",
    "            \n",
    "            item['text'] = header\n",
    "            item['field'] = 'content'\n",
    "            item['name'] = 'Paragraph'\n",
    "            \n",
    "            if len(content['images']['image_urls']) > 0:\n",
    "                item['im_src'] = content['images']['image_urls'][0]\n",
    "            elif thumbnail:\n",
    "                item['im_src'] = im_src\n",
    "            else:\n",
    "                item['im_src'] = ''\n",
    "        \n",
    "            texts.append(item)\n",
    "        \n",
    "        item = {}\n",
    "        text = clean(content['text'])\n",
    "        if len(text) > 0:\n",
    "            item['field'] = 'content'\n",
    "            item['name'] = 'Paragraph'\n",
    "            item['text'] = text\n",
    "            \n",
    "            if len(content['images']['image_urls']) > 0:\n",
    "                item['im_src'] = content['images']['image_urls'][0]\n",
    "            elif thumbnail:\n",
    "                item['im_src'] = im_src\n",
    "            else:\n",
    "                item['im_src'] = ''\n",
    "            \n",
    "            texts.append(item)\n",
    "        \n",
    "        for url, caption in zip(content['images']['image_urls'], content['images']['image_captions']):\n",
    "            item = {}\n",
    "            caption = clean(caption)\n",
    "\n",
    "            if len(caption) > 0:\n",
    "                if len(header) > 0:\n",
    "                    item['text'] = clean(header + ' - ' + caption)\n",
    "                else:\n",
    "                    item['text'] = clean(caption)\n",
    "                item['field'] = 'image'\n",
    "                item['name'] = 'Image'\n",
    "                item['im_src'] = url\n",
    "                texts.append(item)\n",
    "\n",
    "            if len(url) > 0:\n",
    "                images.append(url)\n",
    "    \n",
    "    return texts, images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oklahome State data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 1000\n",
    "\n",
    "'''\n",
    "Oklahome State University data transformation.\n",
    "\n",
    "Data is fetched from 'data/okstate/fact-sheets-out-cleaner.json' file.\n",
    "The transformed data is saved in 'data/transformed/okstate.json' file  \n",
    "'''\n",
    "FILE_PATH = Path.joinpath(Path().absolute().parents[0], 'data/okstate/fact-sheets-out-cleaner.json')\n",
    "SAVE_FILE = Path.joinpath(Path().absolute().parents[0], 'data/transformed/okstate.json')\n",
    "SAVE_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not FILE_PATH.is_file():\n",
    "    raise FileNotFoundError(\n",
    "        (\n",
    "            'File \\'data/okstate/fact-sheets-out-cleaner.json\\' not available.'\n",
    "            ' File folder is empty or not created. Make sure to create data folder.'\n",
    "            ' Follow the instruction in the \\'README-es-ingesting-data.md\\' file.'\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "print(f'Transforming Oklahome State University data...')\n",
    "df = pd.read_json(FILE_PATH)\n",
    "df['source'] = 'okstate'\n",
    "df['title'] = df['title'].apply(clean).fillna('Auxilary')\n",
    "df.rename(columns={'link': 'url'}, inplace=True)\n",
    "df.drop(columns=['author', 'pubdate', 'category', 'displaydate'], inplace=True)\n",
    "texts = []\n",
    "images = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    title_description_texts = get_title_and_description(row, thumbnail=True)\n",
    "    row_texts, row_images = get_contents_and_images(row, thumbnail=True)\n",
    "    title_description_texts.extend(row_texts)\n",
    "    texts.append(row_texts)\n",
    "    images.append(row_images)\n",
    "\n",
    "df['texts'] = texts\n",
    "df['images'] = images\n",
    "df = df.loc[:, ['source', 'url', 'title', 'texts', 'images']]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "if limit:\n",
    "    df = df.sample(limit) if limit < df.shape[0] else df\n",
    "\n",
    "print(f'Final shape is :{df.shape}')\n",
    "df.to_json(SAVE_FILE, orient='records')\n",
    "print(f'Saved file to {SAVE_FILE}')\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oregon State data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 1000\n",
    "\n",
    "'''\n",
    "Oregon State University data transformation.\n",
    "\n",
    "Data is fetched from 'data/orstate/OSU-Out-Cleaner.json' file.\n",
    "The transformed data is saved in 'data/transformed/orstate.json' file  \n",
    "'''\n",
    "FILE_PATH = Path.joinpath(Path().absolute().parents[0], 'data/orstate/OSU-Out-Cleaner.json')\n",
    "SAVE_FILE = Path.joinpath(Path().absolute().parents[0], 'data/transformed/orstate.json')\n",
    "SAVE_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not FILE_PATH.is_file():\n",
    "    raise FileNotFoundError(\n",
    "        (\n",
    "            'File \\'data/orstate/OSU-Out-Cleaner.json\\' not available.'\n",
    "            ' File folder is empty or not created. Make sure to create data folder.'\n",
    "            ' Follow the instruction in the \\'README-es-ingesting-data.md\\' file.'\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(f'Transforming Oregon State University data...')\n",
    "df = pd.read_json(FILE_PATH)\n",
    "df['source'] = 'orstate'\n",
    "df['title'] = df['title'].apply(clean).fillna('Auxilary')\n",
    "df.rename(columns={'link': 'url'}, inplace=True)\n",
    "df.drop(columns=['author', 'pubdate', 'category', 'displaydate'], inplace=True)\n",
    "texts = []\n",
    "images = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    title_description_texts = get_title_and_description(row)\n",
    "    row_texts, row_images = get_contents_and_images(row)\n",
    "    title_description_texts.extend(row_texts)\n",
    "    texts.append(row_texts)\n",
    "    images.append(row_images)\n",
    "    \n",
    "df['texts'] = texts\n",
    "df['images'] = images\n",
    "df = df.loc[:, ['source', 'url', 'title', 'texts', 'images']]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "if limit:\n",
    "    df = df.sample(limit) if limit < df.shape[0] else df\n",
    "\n",
    "print(f'Final shape is :{df.shape}')\n",
    "df.to_json(SAVE_FILE, orient='records')\n",
    "print(f'Saved file to {SAVE_FILE}')\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('es-data-ingestion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "607e550c5f7577ecefb8f11c45030a36424a53b9c08a45019262888187990ca7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
